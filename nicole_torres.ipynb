{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n 2 ‚Äì Data Science para la Energ√≠a Solar\n",
    "\n",
    "Alumna: Nicole Torres\n",
    "\n",
    "Prof: Cristobal Parrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n\n",
    "Este notebook presenta el desarrollo completo de la Evaluaci√≥n 2 del curso Data Science para la Energ√≠a Solar.\n",
    "Se eval√∫a t√©cnica y econ√≥micamente el desempe√±o de una planta fotovoltaica de 50 MWDC en tres localidades:\n",
    "Calama, Salvador y Vallenar, usando datos TMY. Se incluyen limpieza de datos, simulaciones\n",
    "con PySAM, an√°lisis de LCOE y VAN, estudio de sensibilidad y un dashboard interactivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58eebe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "#  DESCRIPCI√ìN E IMPORTACI√ìN DE LIBRER√çAS\n",
    "# =============================================================\n",
    "#\n",
    "# Autor: Nicole Torres\n",
    "# Descripci√≥n: Este script procesa, limpia y analiza archivos de datos horarios solares (GHI, DNI, DHI)\n",
    "#              para diferentes localidades, generando archivos TMY artificiales, reportes EDA y gr√°ficos.\n",
    "#              El flujo incluye transformaci√≥n, limpieza avanzada, reconstrucci√≥n de fechas originales,\n",
    "#              llenado jer√°rquico de NaNs y generaci√≥n de informes comparativos.\n",
    "#\n",
    "# Uso:\n",
    "#   1. Coloca los archivos *_corrupted.csv en la carpeta del proyecto.\n",
    "#   2. Ejecuta este script:\n",
    "#        python3 limpieza.py\n",
    "#   3. Los resultados se guardar√°n en las carpetas 'datos_limpios/', 'reports/' y 'plots/'.\n",
    "#\n",
    "# Requiere: pandas, numpy, matplotlib, pvlib, PySAM\n",
    "# =============================================================\n",
    "# =============================\n",
    "# üì¶ Importaci√≥n de librer√≠as\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pvlib\n",
    "import os\n",
    "from pvlib.solarposition import get_solarposition\n",
    "import PySAM.Pvwattsv7 as pv\n",
    "import PySAM.Lcoefcr as Lcoefcr\n",
    "\n",
    "\n",
    "# === CONFIGURACI√ìN DE DIRECTORIOS ===\n",
    "DATOS_LIMPIOS_DIR = Path('datos_limpios')\n",
    "DATOS_LIMPIOS_DIR.mkdir(exist_ok=True)\n",
    "RESULTADOS_PV_DIR = Path('resultados_pv')\n",
    "RESULTADOS_PV_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7d55b",
   "metadata": {},
   "source": [
    "## 2. Carga y limpieza de datos\n",
    "\n",
    "Se trabaj√≥ con archivos meteorol√≥gicos por localidad (Calama, Salvador, Vallenar), cada uno con 43.824 registros horarios de un archivo TMY. Aproximadamente un 8 % de los datos estaban corruptos, incluyendo:\n",
    "\n",
    "- Valores nulos (NaN), ceros, y negativos en columnas clave como GHI, DNI, DHI, TempC y Wind_mps.\n",
    "- Outliers detectados seg√∫n umbrales f√≠sicos razonables.\n",
    "\n",
    "La estrategia aplicada consisti√≥ en:\n",
    "\n",
    "1. Cargar los archivos y evaluar calidad de datos.\n",
    "2. Reemplazar valores negativos y nulos con NaN.\n",
    "3. Aplicar interpolaci√≥n lineal cuando los huecos eran menores o iguales a 3 pasos.\n",
    "4. Validar la limpieza con gr√°ficas comparativas antes/despu√©s y estad√≠sticas b√°sicas.\n",
    "\n",
    "El objetivo fue mantener los 43.824 registros por archivo, asegurando consistencia temporal y viabilidad f√≠sica de los datos para simulaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4556f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento...\n",
      "\n",
      "Procesando Salvador...\n",
      "Leyendo archivo: salvador_corrupted.csv\n",
      "Transformaci√≥n completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9865/4201889616.py:383: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - Salvador ===\n",
      "GHI: 111 outliers, 1 NaNs\n",
      "  - Negativos: 0\n",
      "  - Altos: 111\n",
      "  - NaNs: 1\n",
      "DNI: 0 outliers, 0 NaNs\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "‚úÖ Archivo limpio guardado en: datos_limpios/salvador_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gr√°ficos...\n",
      "Realizando an√°lisis estacional...\n",
      "\n",
      "=== AN√ÅLISIS ESTACIONAL - Salvador ===\n",
      "==================================================\n",
      "\n",
      "Estad√≠sticas por Estaci√≥n:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI M√°ximo DNI M√°ximo  \\\n",
      "Verano      389.684931   541.464676    15.965579     1248.3     1222.7   \n",
      "Oto√±o       258.563072   406.613972    14.172056     1167.7     1203.6   \n",
      "Invierno    197.343727   337.040602    11.540444      978.0     1149.7   \n",
      "Primavera   354.186401   515.705128    13.635943     1246.4     1223.6   \n",
      "\n",
      "          DHI M√°ximo Horas de Sol Energ√≠a Total  \n",
      "Verano         518.2         1201     841.71945  \n",
      "Oto√±o          535.7         1039      570.6487  \n",
      "Invierno       164.5          970     435.73495  \n",
      "Primavera      177.5         1143      773.5431  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Salvador completado.\n",
      "\n",
      "Procesando Calama...\n",
      "Leyendo archivo: calama_corrupted.csv\n",
      "Transformaci√≥n completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9865/4201889616.py:383: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - Calama ===\n",
      "GHI: 49 outliers, 3 NaNs\n",
      "  - Negativos: 0\n",
      "  - Altos: 49\n",
      "  - NaNs: 3\n",
      "DNI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "‚úÖ Archivo limpio guardado en: datos_limpios/calama_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gr√°ficos...\n",
      "Realizando an√°lisis estacional...\n",
      "\n",
      "=== AN√ÅLISIS ESTACIONAL - Calama ===\n",
      "==================================================\n",
      "\n",
      "Estad√≠sticas por Estaci√≥n:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI M√°ximo DNI M√°ximo  \\\n",
      "Verano       371.25412   484.361458    16.991319     1254.0     1192.9   \n",
      "Oto√±o        276.07951    425.86216    14.890082     1156.1     1163.1   \n",
      "Invierno    223.032465     379.3744    11.958243      985.3     1127.4   \n",
      "Primavera   351.340911   488.193816    17.601717     1233.8     1196.4   \n",
      "\n",
      "          DHI M√°ximo Horas de Sol Energ√≠a Total  \n",
      "Verano         432.1         1196      801.9089  \n",
      "Oto√±o          349.8         1060      609.0314  \n",
      "Invierno       192.0          987     492.23265  \n",
      "Primavera      527.6         1135     767.32855  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Calama completado.\n",
      "\n",
      "Procesando Vallenar...\n",
      "Leyendo archivo: Vallenar_corrupted.csv\n",
      "Transformaci√≥n completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9865/4201889616.py:383: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - Vallenar ===\n",
      "GHI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DNI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "‚úÖ Archivo limpio guardado en: datos_limpios/vallenar_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gr√°ficos...\n",
      "Realizando an√°lisis estacional...\n",
      "\n",
      "=== AN√ÅLISIS ESTACIONAL - Vallenar ===\n",
      "==================================================\n",
      "\n",
      "Estad√≠sticas por Estaci√≥n:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI M√°ximo DNI M√°ximo  \\\n",
      "Verano      352.003056   464.676795    21.866458     1152.6     1111.0   \n",
      "Oto√±o        217.90401   326.889253    16.282994     1049.1     1074.7   \n",
      "Invierno    156.989964   266.652447    14.600317      864.3     1036.3   \n",
      "Primavera   302.854806   393.443402    18.393246     1164.0     1132.5   \n",
      "\n",
      "          DHI M√°ximo Horas de Sol Energ√≠a Total  \n",
      "Verano         268.5         1181      760.3266  \n",
      "Oto√±o          260.5         1029     480.91415  \n",
      "Invierno       332.6          961     346.47685  \n",
      "Primavera      235.0         1149    661.434897  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Vallenar completado.\n",
      "\n",
      "Generando resumen comparativo...\n",
      "\n",
      "=== RESUMEN COMPARATIVO DE DATOS SOLARES ===\n",
      "==================================================\n",
      "\n",
      "An√°lisis de Valores Faltantes:\n",
      "\n",
      "Salvador:\n",
      "  GHI: 1 valores faltantes (0.01%)\n",
      "  DNI: 0 valores faltantes (0.00%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Calama:\n",
      "  GHI: 3 valores faltantes (0.03%)\n",
      "  DNI: 2 valores faltantes (0.02%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Vallenar:\n",
      "  GHI: 2 valores faltantes (0.02%)\n",
      "  DNI: 2 valores faltantes (0.02%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "An√°lisis de Outliers:\n",
      "\n",
      "Salvador:\n",
      "  GHI: 111 outliers (1.27%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 111\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 5 outliers (0.06%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 5\n",
      "\n",
      "Calama:\n",
      "  GHI: 49 outliers (0.56%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 49\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 4 outliers (0.05%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 4\n",
      "\n",
      "Vallenar:\n",
      "  GHI: 0 outliers (0.00%)\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 0 outliers (0.00%)\n",
      "\n",
      "Reconstruyendo archivos TMY limpios con fechas originales...\n",
      "‚úÖ Archivo TMY limpio con fechas originales guardado en: datos_limpios/salvador_TMY_limpio_originales.csv\n",
      "‚úÖ Archivo TMY limpio con fechas originales guardado en: datos_limpios/calama_TMY_limpio_originales.csv\n",
      "‚úÖ Archivo TMY limpio con fechas originales guardado en: datos_limpios/vallenar_TMY_limpio_originales.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/calama_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 3 NaNs, Max gap: 1 hours\n",
      "DNI: 2 NaNs, Max gap: 1 hours\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/calama_TMY_limpio_originales.csv despu√©s de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "‚úÖ Archivo final guardado en: datos_limpios/calama_TMY_final.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/salvador_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 1 NaNs, Max gap: 1 hours\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/salvador_TMY_limpio_originales.csv despu√©s de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "‚úÖ Archivo final guardado en: datos_limpios/salvador_TMY_final.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/vallenar_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 2 NaNs, Max gap: 1 hours\n",
      "DNI: 2 NaNs, Max gap: 1 hours\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/vallenar_TMY_limpio_originales.csv despu√©s de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "‚úÖ Archivo final guardado en: datos_limpios/vallenar_TMY_final.csv\n",
      "[DEBUG] Generando reporte EDA en: reports/calama_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Calama...\n",
      "[DEBUG] Generando reporte EDA en: reports/salvador_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Salvador...\n",
      "[DEBUG] Generando reporte EDA en: reports/vallenar_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Vallenar...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "#  LIMPIEZA Y AN√ÅLISIS DE DATOS SOLARES TMY PARA SIMULACI√ìN PV\n",
    "# =============================================================\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# =============================================================\n",
    "# FUNCIONES PRINCIPALES DEL FLUJO DE LIMPIEZA Y AN√ÅLISIS\n",
    "# =============================================================\n",
    "\n",
    "def transformar_a_tmy_con_metadatos(csv_path, output_path, metadata_dict):\n",
    "    \"\"\"\n",
    "    Transforma un archivo CSV con datos horarios en un archivo TMY artificial con metadatos en el encabezado.\n",
    "    - Ordena cronol√≥gicamente, recorta a 8760 filas, asigna a√±o artificial y reordena columnas.\n",
    "    - Escribe las tres primeras l√≠neas como metadatos y encabezados.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Eliminar columna 'datetime' si existe\n",
    "    if 'datetime' in df.columns:\n",
    "        df = df.drop(columns=['datetime'])\n",
    "\n",
    "    # Ordenar cronol√≥gicamente y recortar o completar a 8760 filas\n",
    "    df = df.sort_values(by=[\"Month\", \"Day\", \"Hour\", \"Minute\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.iloc[:8760]  # en caso de que tenga m√°s filas\n",
    "\n",
    "    # Asignar un a√±o artificial constante\n",
    "    df['Year'] = 1990\n",
    "\n",
    "    # Reordenar columnas: primero las de fecha\n",
    "    columnas_fecha = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    otras = [c for c in df.columns if c not in columnas_fecha]\n",
    "    df = df[columnas_fecha + otras]\n",
    "\n",
    "    # Escribir archivo con las tres primeras l√≠neas de metadatos\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # L√≠nea 1: encabezados de metadatos\n",
    "        f.write(\"Source,Location ID,City,State,Country,Latitude,Longitude,Time Zone,Elevation\\n\")\n",
    "        # L√≠nea 2: valores de metadatos\n",
    "        f.write(\",\".join(str(metadata_dict[k]) for k in [\n",
    "            \"Source\", \"Location ID\", \"City\", \"State\", \"Country\",\n",
    "            \"Latitude\", \"Longitude\", \"Time Zone\", \"Elevation\"\n",
    "        ]) + \"\\n\")\n",
    "        # L√≠nea 3: encabezado de columnas de datos\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        # Resto de datos\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_tmy_data(df, location, output_dir=\"plots\", is_clean=False):\n",
    "    \"\"\"\n",
    "    Genera y guarda gr√°ficos de GHI, DNI y DHI para una localidad.\n",
    "    - Los gr√°ficos se guardan en la carpeta 'plots/'.\n",
    "    \"\"\"\n",
    "    # Crear directorio para gr√°ficos si no existe\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Configurar estilo de gr√°ficos\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Crear figura con tres subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))\n",
    "    title_suffix = \" (Datos Limpios)\" if is_clean else \"\"\n",
    "    fig.suptitle(f'Radiaci√≥n Solar por Hora - {location}{title_suffix}', fontsize=16, y=0.95)\n",
    "\n",
    "    # Crear fechas para el eje x\n",
    "    dates = [datetime(1990, int(row['Month']), int(row['Day']), int(row['Hour'])) \n",
    "             for _, row in df.iterrows()]\n",
    "\n",
    "    # Graficar cada componente en su propio subplot\n",
    "    ax1.plot(dates, df['GHI'], 'b-', linewidth=1, alpha=0.7)\n",
    "    ax1.set_title('Radiaci√≥n Global Horizontal (GHI)', fontsize=12)\n",
    "    ax1.set_ylabel('GHI (W/m¬≤)', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1300)  # Establecer l√≠mite superior para GHI\n",
    "\n",
    "    ax2.plot(dates, df['DNI'], 'r-', linewidth=1, alpha=0.7)\n",
    "    ax2.set_title('Radiaci√≥n Normal Directa (DNI)', fontsize=12)\n",
    "    ax2.set_ylabel('DNI (W/m¬≤)', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1300)  # Establecer l√≠mite superior para DNI\n",
    "\n",
    "    ax3.plot(dates, df['DHI'], 'g-', linewidth=1, alpha=0.7)\n",
    "    ax3.set_title('Radiaci√≥n Horizontal Difusa (DHI)', fontsize=12)\n",
    "    ax3.set_ylabel('DHI (W/m¬≤)', fontsize=10)\n",
    "    ax3.set_xlabel('Mes', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 600)  # Establecer l√≠mite superior para DHI\n",
    "\n",
    "    # Configurar formato del eje x para todos los subplots\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Ajustar layout y guardar\n",
    "    plt.tight_layout()\n",
    "    suffix = \"_clean\" if is_clean else \"\"\n",
    "    plt.savefig(f'{output_dir}/{location.lower()}_tmy_plots{suffix}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_eda_report(df, location, output_dir=\"reports\", df_final=None):\n",
    "    \"\"\"\n",
    "    Genera un informe EDA (An√°lisis Exploratorio de Datos) con:\n",
    "    - Estad√≠sticas de valores faltantes y outliers\n",
    "    - Estad√≠sticas descriptivas\n",
    "    - Recomendaciones de limpieza\n",
    "    - (Opcional) Resumen num√©rico de la limpieza final si se pasa df_final\n",
    "    El informe se guarda en 'reports/{localidad}_eda_report.txt'.\n",
    "    \"\"\"\n",
    "    # Crear directorio para informes si no existe\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Crear archivo de informe\n",
    "    report_path = Path(output_dir) / f\"{location.lower()}_eda_report.txt\"\n",
    "    print(f\"[DEBUG] Generando reporte EDA en: {report_path}\")\n",
    "\n",
    "    # Definir l√≠mites de outliers\n",
    "    outlier_limits = {\n",
    "        'GHI': 1200,  # Actualizado para coincidir con limpiar_TMY_completo\n",
    "        'DNI': 1300,\n",
    "        'DHI': 600\n",
    "    }\n",
    "\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"=== Informe EDA - {location} ===\\n\\n\")\n",
    "\n",
    "        # 1. Informaci√≥n general\n",
    "        f.write(\"1. INFORMACI√ìN GENERAL\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"N√∫mero total de registros: {len(df)}\\n\")\n",
    "        f.write(f\"Per√≠odo: {df['Month'].min()}/{df['Day'].min()} - {df['Month'].max()}/{df['Day'].max()}\\n\\n\")\n",
    "\n",
    "        # 2. An√°lisis de valores faltantes\n",
    "        f.write(\"2. AN√ÅLISIS DE VALORES FALTANTES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        nan_counts = df[['GHI', 'DNI', 'DHI']].isna().sum()\n",
    "        nan_percentages = (nan_counts / len(df)) * 100\n",
    "\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            f.write(f\"  - N√∫mero de valores faltantes: {nan_counts[col]}\\n\")\n",
    "            f.write(f\"  - Porcentaje de valores faltantes: {nan_percentages[col]:.2f}%\\n\")\n",
    "\n",
    "            # An√°lisis de secuencias de NaN\n",
    "            if nan_counts[col] > 0:\n",
    "                nan_sequences = df[col].isna().astype(int).groupby(\n",
    "                    (df[col].isna().astype(int).diff() != 0).cumsum()\n",
    "                ).cumsum()\n",
    "                max_consecutive = nan_sequences.max()\n",
    "                f.write(f\"  - M√°xima secuencia de NaN consecutivos: {max_consecutive}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 3. An√°lisis de Outliers\n",
    "        f.write(\"3. AN√ÅLISIS DE OUTLIERS\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(\"Criterios de outliers:\\n\")\n",
    "        f.write(\"- GHI > 1200 W/m¬≤ o < 0\\n\")  # Actualizado\n",
    "        f.write(\"- DNI > 1300 W/m¬≤ o < 0\\n\")\n",
    "        f.write(\"- DHI > 600 W/m¬≤ o < 0\\n\\n\")\n",
    "\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            # Identificar outliers (valores negativos o mayores al l√≠mite)\n",
    "            neg_outliers = df[df[col] < 0][col]\n",
    "            high_outliers = df[df[col] > outlier_limits[col]][col]\n",
    "            total_outliers = len(neg_outliers) + len(high_outliers)\n",
    "\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            f.write(f\"  - N√∫mero total de outliers: {total_outliers}\\n\")\n",
    "            f.write(f\"  - Porcentaje de outliers: {(total_outliers/len(df))*100:.2f}%\\n\")\n",
    "\n",
    "            if len(neg_outliers) > 0:\n",
    "                f.write(f\"  - Outliers negativos: {len(neg_outliers)} ({len(neg_outliers)/total_outliers*100:.2f}% del total de outliers)\\n\")\n",
    "                f.write(f\"    * Valor m√≠nimo: {neg_outliers.min():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Valor m√°ximo negativo: {neg_outliers.max():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Media de outliers negativos: {neg_outliers.mean():.2f} W/m¬≤\\n\")\n",
    "\n",
    "            if len(high_outliers) > 0:\n",
    "                f.write(f\"  - Outliers altos: {len(high_outliers)} ({len(high_outliers)/total_outliers*100:.2f}% del total de outliers)\\n\")\n",
    "                f.write(f\"    * Valor m√≠nimo: {high_outliers.min():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Valor m√°ximo: {high_outliers.max():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Media de outliers altos: {high_outliers.mean():.2f} W/m¬≤\\n\")\n",
    "\n",
    "            # An√°lisis temporal de outliers\n",
    "            if total_outliers > 0:\n",
    "                f.write(\"  - Distribuci√≥n temporal de outliers:\\n\")\n",
    "                for month in range(1, 13):\n",
    "                    month_outliers = len(df[(df['Month'] == month) & \n",
    "                                          ((df[col] < 0) | (df[col] > outlier_limits[col]))])\n",
    "                    if month_outliers > 0:\n",
    "                        f.write(f\"    * Mes {month}: {month_outliers} outliers\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # 4. Estad√≠sticas descriptivas (excluyendo outliers)\n",
    "        f.write(\"4. ESTAD√çSTICAS DESCRIPTIVAS (EXCLUYENDO OUTLIERS)\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            # Filtrar valores v√°lidos (no outliers)\n",
    "            valid_data = df[(df[col] >= 0) & (df[col] <= outlier_limits[col])][col]\n",
    "            stats = valid_data.describe()\n",
    "            f.write(f\"  - Media: {stats['mean']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Desviaci√≥n est√°ndar: {stats['std']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - M√≠nimo: {stats['min']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - M√°ximo: {stats['max']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Mediana: {stats['50%']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Q1 (25%): {stats['25%']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Q3 (75%): {stats['75%']:.2f} W/m¬≤\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 5. Recomendaciones\n",
    "        f.write(\"5. RECOMENDACIONES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            if nan_counts[col] > 0:\n",
    "                f.write(f\"  - Considerar interpolaci√≥n para {nan_counts[col]} valores faltantes \")\n",
    "                if 'max_consecutive' in locals() and max_consecutive > 4:\n",
    "                    f.write(f\"(¬°Alerta! Hay secuencias de hasta {max_consecutive} NaN consecutivos)\\n\")\n",
    "                else:\n",
    "                    f.write(\"(secuencias cortas, adecuadas para interpolaci√≥n)\\n\")\n",
    "\n",
    "            neg_count = len(df[df[col] < 0])\n",
    "            high_count = len(df[df[col] > outlier_limits[col]])\n",
    "            if neg_count > 0 or high_count > 0:\n",
    "                f.write(f\"  - Reemplazar {neg_count + high_count} outliers:\\n\")\n",
    "                if neg_count > 0:\n",
    "                    f.write(f\"    * {neg_count} valores negativos con 0\\n\")\n",
    "                if high_count > 0:\n",
    "                    f.write(f\"    * {high_count} valores > {outlier_limits[col]} W/m¬≤ con {outlier_limits[col]} W/m¬≤\\n\")\n",
    "                f.write(f\"  - Revisar la calidad de los datos en los meses con mayor concentraci√≥n de outliers\\n\")\n",
    "\n",
    "        f.write(\"\\nRecomendaciones generales:\\n\")\n",
    "        f.write(\"1. Reemplazar todos los valores negativos con 0\\n\")\n",
    "        f.write(\"2. Limitar los valores m√°ximos a los umbrales f√≠sicos:\\n\")\n",
    "        f.write(\"   - GHI: 1200 W/m¬≤\\n\")  # Actualizado\n",
    "        f.write(\"   - DNI: 1300 W/m¬≤\\n\")\n",
    "        f.write(\"   - DHI: 600 W/m¬≤\\n\")\n",
    "        f.write(\"3. Considerar la interpolaci√≥n solo para secuencias cortas de NaN (‚â§ 4 horas)\\n\")\n",
    "        f.write(\"4. Revisar la calidad de los datos en los meses con mayor concentraci√≥n de outliers\\n\")\n",
    "        f.write(\"5. Documentar el proceso de limpieza y las decisiones tomadas para el manejo de outliers\\n\")\n",
    "        # --- SECCI√ìN DE RESULTADOS FINALES ---\n",
    "        if df_final is not None:\n",
    "            print(f\"[DEBUG] Escribiendo resultados finales para {location}...\")\n",
    "            f.write(\"\\n=== RESULTADOS FINALES DE LA LIMPIEZA ===\\n\")\n",
    "            outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 600}\n",
    "            for col in ['GHI', 'DNI', 'DHI']:\n",
    "                neg_outliers = len(df_final[df_final[col] < 0])\n",
    "                high_outliers = len(df_final[df_final[col] > outlier_limits[col]])\n",
    "                total_outliers = neg_outliers + high_outliers\n",
    "                nans = df_final[col].isna().sum()\n",
    "                f.write(f\"{col}: {total_outliers} outliers (Negativos: {neg_outliers}, Altos: {high_outliers}), {nans} NaNs\\n\")\n",
    "\n",
    "def analisis_estacional(df, location):\n",
    "    \"\"\"\n",
    "    Realiza un an√°lisis estacional de los datos solares para una localidad.\n",
    "    Imprime estad√≠sticas por estaci√≥n (verano, oto√±o, invierno, primavera).\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== AN√ÅLISIS ESTACIONAL - {location} ===\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Definir estaciones\n",
    "    estaciones = {\n",
    "        'Verano': [12, 1, 2],\n",
    "        'Oto√±o': [3, 4, 5],\n",
    "        'Invierno': [6, 7, 8],\n",
    "        'Primavera': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    # Crear DataFrame para almacenar estad√≠sticas estacionales\n",
    "    stats_estacionales = pd.DataFrame(index=estaciones.keys(), \n",
    "                                    columns=['GHI Promedio', 'DNI Promedio', 'DHI Promedio',\n",
    "                                            'GHI M√°ximo', 'DNI M√°ximo', 'DHI M√°ximo',\n",
    "                                            'Horas de Sol', 'Energ√≠a Total'])\n",
    "\n",
    "    # Calcular estad√≠sticas por estaci√≥n\n",
    "    for estacion, meses in estaciones.items():\n",
    "        df_estacion = df[df['Month'].isin(meses)]\n",
    "\n",
    "        # Estad√≠sticas b√°sicas\n",
    "        stats_estacionales.loc[estacion, 'GHI Promedio'] = df_estacion['GHI'].mean()\n",
    "        stats_estacionales.loc[estacion, 'DNI Promedio'] = df_estacion['DNI'].mean()\n",
    "        stats_estacionales.loc[estacion, 'DHI Promedio'] = df_estacion['DHI'].mean()\n",
    "\n",
    "        stats_estacionales.loc[estacion, 'GHI M√°ximo'] = df_estacion['GHI'].max()\n",
    "        stats_estacionales.loc[estacion, 'DNI M√°ximo'] = df_estacion['DNI'].max()\n",
    "        stats_estacionales.loc[estacion, 'DHI M√°ximo'] = df_estacion['DHI'].max()\n",
    "\n",
    "        # Horas de sol y energ√≠a\n",
    "        stats_estacionales.loc[estacion, 'Horas de Sol'] = len(df_estacion[df_estacion['GHI'] > 0])\n",
    "        stats_estacionales.loc[estacion, 'Energ√≠a Total'] = df_estacion['GHI'].sum() / 1000\n",
    "\n",
    "    # Mostrar estad√≠sticas estacionales\n",
    "    print(\"\\nEstad√≠sticas por Estaci√≥n:\")\n",
    "    print(stats_estacionales.round(2))\n",
    "\n",
    "\n",
    "def marcar_outliers_nan(df):\n",
    "    \"\"\"\n",
    "    Marca como NaN los valores f√≠sicamente inv√°lidos en GHI, DNI y DHI.\n",
    "    - GHI < 0 o GHI > 1400\n",
    "    - DNI < 0 o DNI > 1300\n",
    "    - DHI < 0 o DHI > 600\n",
    "    Devuelve un DataFrame con los valores inv√°lidos como NaN.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['GHI'] = df['GHI'].mask((df['GHI'] < 0) | (df['GHI'] > 1400))\n",
    "    df['DNI'] = df['DNI'].mask((df['DNI'] < 0) | (df['DNI'] > 1300))\n",
    "    df['DHI'] = df['DHI'].mask((df['DHI'] < 0) | (df['DHI'] > 600))\n",
    "    return df\n",
    "\n",
    "\n",
    "def limpiar_TMY_completo(archivo_entrada, archivo_salida, max_ghi=1400, max_dni=1300, max_dhi=600, interp_limit=6, location=None):\n",
    "    \"\"\"\n",
    "    Limpia y valida f√≠sicamente los datos TMY artificiales:\n",
    "    - Aplica l√≠mites f√≠sicos y de temporada a GHI, DNI, DHI\n",
    "    - Interpola NaNs de forma robusta\n",
    "    - Valida DHI con posici√≥n solar\n",
    "    - Limpia Tdry, Tdew, RH y Pres\n",
    "    - Guarda el archivo limpio con metadatos\n",
    "    \"\"\"\n",
    "    # Leer metadatos\n",
    "    with open(archivo_entrada, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(2)]\n",
    "    # Leer datos desde la tercera l√≠nea\n",
    "    df = pd.read_csv(archivo_entrada, skiprows=2)\n",
    "    # Crear columna datetime y usar como √≠ndice\n",
    "    df[\"datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # Ajustar l√≠mites seg√∫n la ubicaci√≥n\n",
    "    if location == \"Vallenar\":\n",
    "        max_ghi = 1200\n",
    "        ghi_high_season = 1150\n",
    "    else:\n",
    "        max_ghi = 1400\n",
    "        ghi_high_season = 1250\n",
    "\n",
    "    # Limpiar GHI con l√≠mites dependientes del mes y ubicaci√≥n\n",
    "    df[\"GHI\"] = df[\"GHI\"].mask(\n",
    "        ((df[\"Month\"].between(3, 10)) & (df[\"GHI\"] >= ghi_high_season)) |  # L√≠mite para meses de alta radiaci√≥n\n",
    "        ((~df[\"Month\"].between(3, 10)) & (df[\"GHI\"] > max_ghi)) |  # L√≠mite general\n",
    "        (df[\"GHI\"] < 0)\n",
    "    )\n",
    "    df[\"GHI\"] = df[\"GHI\"].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Limpiar DNI\n",
    "    df[\"DNI\"] = df[\"DNI\"].mask((df[\"DNI\"] < 0) | (df[\"DNI\"] > max_dni))\n",
    "    df[\"DNI\"] = df[\"DNI\"].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Limpiar DHI con chequeos f√≠sicos avanzados\n",
    "    # Calcular posici√≥n solar para validaci√≥n f√≠sica\n",
    "    lat = -26.2533 if location == \"Salvador\" else -22.4661 if location == \"Calama\" else -28.5766\n",
    "    lon = -69.0522 if location == \"Salvador\" else -68.9244 if location == \"Calama\" else -70.7601\n",
    "    solar_position = get_solarposition(df.index, latitude=lat, longitude=lon)\n",
    "    cos_zenith = np.cos(np.radians(solar_position[\"zenith\"]))\n",
    "    dhi_est = (df[\"GHI\"] - df[\"DNI\"] * cos_zenith).clip(lower=0)\n",
    "\n",
    "    cond_invalid_dhi = (\n",
    "        (df[\"DHI\"] < 0) |\n",
    "        (df[\"DHI\"] > max_dhi) |\n",
    "        (df[\"DHI\"] > df[\"GHI\"]) |\n",
    "        (df[\"DHI\"] > dhi_est + 30) |\n",
    "        (df[\"DHI\"] > 0.95 * df[\"GHI\"]) |\n",
    "        ((solar_position[\"zenith\"] > 90) & (df[\"DHI\"] > 5))\n",
    "    )\n",
    "    df.loc[cond_invalid_dhi, \"DHI\"] = np.nan\n",
    "\n",
    "    # Interpolaci√≥n robusta para DHI\n",
    "    def interpolar_robusto(serie, limit):\n",
    "        nan_groups = serie.isna().astype(int).groupby(serie.notna().astype(int).cumsum()).sum()\n",
    "        if (nan_groups > limit).any():\n",
    "            mask = serie.isna()\n",
    "            for idx, size in nan_groups[nan_groups > limit].items():\n",
    "                mask[mask.groupby(mask.cumsum()).ngroup() == idx] = False\n",
    "            serie_interp = serie.interpolate(method='linear', limit=limit, limit_direction='both')\n",
    "            serie[mask] = serie_interp[mask]\n",
    "            return serie\n",
    "        else:\n",
    "            return serie.interpolate(method='linear', limit=limit, limit_direction='both')\n",
    "\n",
    "    df[\"DHI\"] = interpolar_robusto(df[\"DHI\"], interp_limit)\n",
    "\n",
    "    # Limpiar Tdry, Tdew, RH y Pres\n",
    "    # Definir l√≠mites m√°s realistas para cada par√°metro\n",
    "    temp_limits = {\n",
    "        'Tdry': (-10, 50),  # Limitar entre -10¬∞C y 50¬∞C\n",
    "        'Tdew': (-10, 50),  # Limitar entre -10¬∞C y 50¬∞C\n",
    "        'RH': (1, 100),     # Forzar al rango f√≠sico [1%, 100%]\n",
    "        'Pres': (760, 790)  # Limitar entre 760 y 790 hPa\n",
    "    }\n",
    "\n",
    "    for param, (min_val, max_val) in temp_limits.items():\n",
    "        df[param] = df[param].mask((df[param] < min_val) | (df[param] > max_val))\n",
    "        df[param] = df[param].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Interpolaci√≥n para corregir NaNs\n",
    "    for col in ['GHI', 'DNI', 'DHI', 'Tdry', 'Tdew', 'RH', 'Pres']:\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Opcional: Rellenar cualquier NaN restante con la media de la columna\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Restaurar columnas separadas\n",
    "    df = df.reset_index()\n",
    "    df[\"Year\"] = df[\"datetime\"].dt.year\n",
    "    df[\"Month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"Day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"Hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"Minute\"] = df[\"datetime\"].dt.minute\n",
    "\n",
    "    # Reordenar columnas\n",
    "    columnas_fecha = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]\n",
    "    columnas_finales = columnas_fecha + [col for col in df.columns if col not in columnas_fecha + [\"datetime\"]]\n",
    "    df = df[columnas_finales]\n",
    "\n",
    "    # --- NUEVOS OUTLIERS ESPEC√çFICOS POR UBICACI√ìN Y MES ---\n",
    "    if location == \"Vallenar\":\n",
    "        # GHI > 1000 entre abril y agosto\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(4, 8)) & (df[\"GHI\"] > 1000))\n",
    "        # DNI > 1150 todo el a√±o\n",
    "        df[\"DNI\"] = df[\"DNI\"].mask(df[\"DNI\"] > 1150)\n",
    "    elif location == \"Calama\":\n",
    "        # GHI > 1000 entre mayo y julio\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(5, 7)) & (df[\"GHI\"] > 1000))\n",
    "        # DNI > 1200 todo el a√±o\n",
    "        df[\"DNI\"] = df[\"DNI\"].mask(df[\"DNI\"] > 1200)\n",
    "    elif location == \"Salvador\":\n",
    "        # GHI > 1100 entre abril y agosto\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(4, 8)) & (df[\"GHI\"] > 1100))\n",
    "    # --- FIN NUEVOS OUTLIERS ---\n",
    "\n",
    "    # Despu√©s de limpiar los datos\n",
    "    df_limpio = df  # Aseg√∫rate de que df_limpio est√© definido\n",
    "    revisar_outliers_final(df_limpio, location)\n",
    "\n",
    "    # Guardar nuevo archivo con metadatos originales\n",
    "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "    print(f\"‚úÖ Archivo limpio guardado en: {archivo_salida}\")\n",
    "\n",
    "\n",
    "def revisar_outliers_final(df, location):\n",
    "    \"\"\"\n",
    "    Imprime en consola el resumen de outliers y NaNs tras la limpieza final.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - {location} ===\")\n",
    "    outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 600}  # L√≠mites de outliers\n",
    "\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        neg_outliers = len(df[df[col] < 0])\n",
    "        high_outliers = len(df[df[col] > outlier_limits[col]])\n",
    "        total_outliers = neg_outliers + high_outliers\n",
    "        nans = df[col].isna().sum()\n",
    "        print(f\"{col}: {total_outliers} outliers, {nans} NaNs\")\n",
    "        if total_outliers > 0:\n",
    "            print(f\"  - Negativos: {neg_outliers}\")\n",
    "            print(f\"  - Altos: {high_outliers}\")\n",
    "        if nans > 0:\n",
    "            print(f\"  - NaNs: {nans}\")\n",
    "\n",
    "\n",
    "def generar_resumen_comparativo(dfs, locations):\n",
    "    \"\"\"\n",
    "    Imprime en consola un resumen comparativo de NaNs y outliers para todas las localidades.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RESUMEN COMPARATIVO DE DATOS SOLARES ===\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # An√°lisis de valores faltantes\n",
    "    print(\"\\nAn√°lisis de Valores Faltantes:\")\n",
    "    for df, location in zip(dfs, locations):\n",
    "        nan_counts = df[['GHI', 'DNI', 'DHI']].isna().sum()\n",
    "        print(f\"\\n{location}:\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            print(f\"  {col}: {nan_counts[col]} valores faltantes ({(nan_counts[col]/len(df))*100:.2f}%)\")\n",
    "\n",
    "    # An√°lisis de outliers\n",
    "    print(\"\\nAn√°lisis de Outliers:\")\n",
    "    outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 400}  # Actualizado\n",
    "    for df, location in zip(dfs, locations):\n",
    "        print(f\"\\n{location}:\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            neg_outliers = len(df[df[col] < 0])\n",
    "            high_outliers = len(df[df[col] > outlier_limits[col]])\n",
    "            total_outliers = neg_outliers + high_outliers\n",
    "            print(f\"  {col}: {total_outliers} outliers ({(total_outliers/len(df))*100:.2f}%)\")\n",
    "            if total_outliers > 0:\n",
    "                print(f\"    - Negativos: {neg_outliers}\")\n",
    "                print(f\"    - Altos: {high_outliers}\")\n",
    "\n",
    "\n",
    "def reconstruir_TMY_con_fechas_originales(archivo_corrupto, archivo_limpio, archivo_salida):\n",
    "    \"\"\"\n",
    "    Reconstruye un archivo TMY limpio usando las fechas originales del archivo fuente.\n",
    "    Mantiene los metadatos y el formato TMY.\n",
    "    \"\"\"\n",
    "    # Leer datos limpios\n",
    "    df_limpio = pd.read_csv(archivo_limpio, skiprows=2)\n",
    "    # Leer datos originales (sin metadatos)\n",
    "    df_original = pd.read_csv(archivo_corrupto)\n",
    "    # Reemplazar columnas de fecha por las originales\n",
    "    for col in ['Year', 'Month', 'Day', 'Hour', 'Minute']:\n",
    "        if col in df_original.columns and col in df_limpio.columns:\n",
    "            df_limpio[col] = df_original[col].values\n",
    "    # Reordenar columnas para mantener el formato\n",
    "    columnas_fecha = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    columnas_finales = columnas_fecha + [col for col in df_limpio.columns if col not in columnas_fecha]\n",
    "    df_limpio = df_limpio[columnas_finales]\n",
    "    # Leer metadatos (primeras 3 l√≠neas del archivo limpio)\n",
    "    with open(archivo_limpio, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(3)]\n",
    "    # Guardar el nuevo archivo\n",
    "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        df_limpio.to_csv(f, index=False, header=False)\n",
    "    print(f\"‚úÖ Archivo TMY limpio con fechas originales guardado en: {archivo_salida}\")\n",
    "\n",
    "\n",
    "def scan_nan_gaps(df):\n",
    "    \"\"\"\n",
    "    Muestra en consola cu√°ntos NaN y los huecos m√°s largos en GHI, DNI, DHI.\n",
    "    \"\"\"\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            nan_sequences = df[col].isna().astype(int).groupby(\n",
    "                (df[col].isna().astype(int).diff() != 0).cumsum()\n",
    "            ).cumsum()\n",
    "            max_consecutive = nan_sequences.max()\n",
    "            print(f\"{col}: {nan_count} NaNs, Max gap: {max_consecutive} hours\")\n",
    "        else:\n",
    "            print(f\"{col}: No NaNs\")\n",
    "\n",
    "\n",
    "def fill_nan_hierarchical(df):\n",
    "    \"\"\"\n",
    "    Llena NaNs jer√°rquicamente:\n",
    "    1. Interpola huecos ‚â§ 3 h\n",
    "    2. Rellena lo restante con el percentil 75 Mes-Hora\n",
    "    3. Si a√∫n falta, con el percentil 75 anual por Hora\n",
    "    \"\"\"\n",
    "    # Interpolaci√≥n para huecos peque√±os\n",
    "    df.interpolate(method='linear', limit=3, limit_direction='both', inplace=True)\n",
    "\n",
    "    # Rellenar con percentil 75 Mes-Hora\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df.groupby(['Month', 'Hour'])[col].transform(lambda x: x.fillna(x.quantile(0.75)))\n",
    "\n",
    "    # Rellenar con percentil 75 anual por Hora\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df.groupby('Hour')[col].transform(lambda x: x.fillna(x.quantile(0.75)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_and_save_final_tmy(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Aplica el llenado jer√°rquico de NaNs y guarda el archivo final TMY.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(2)]  # Leer las dos primeras l√≠neas de metadatos\n",
    "    df = pd.read_csv(file_path, skiprows=2)\n",
    "\n",
    "    # Poner la columna datetime como √≠ndice\n",
    "    df['datetime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # Escanear huecos de NaN\n",
    "    print(f\"\\nEscaneando huecos de NaN en {file_path} antes de llenar:\")\n",
    "    scan_nan_gaps(df)\n",
    "\n",
    "    # Llenar NaN jer√°rquicamente\n",
    "    df = fill_nan_hierarchical(df)\n",
    "\n",
    "    # Escanear nuevamente para confirmar que no quedan NaN\n",
    "    print(f\"\\nEscaneando huecos de NaN en {file_path} despu√©s de llenar:\")\n",
    "    scan_nan_gaps(df)\n",
    "\n",
    "    # Guardar el resultado con el mismo formato TMY\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        f.write(','.join(df.columns) + '\\n')  # Escribir encabezado de columnas\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "    print(f\"‚úÖ Archivo final guardado en: {output_path}\")\n",
    "\n",
    "# =============================================================\n",
    "# FLUJO PRINCIPAL DE EJECUCI√ìN\n",
    "# =============================================================\n",
    "\n",
    "def main_tmy():\n",
    "    \"\"\"\n",
    "    Ejecuta el flujo completo de limpieza, an√°lisis y generaci√≥n de reportes para las tres localidades.\n",
    "    - Transforma y limpia los archivos originales\n",
    "    - Genera gr√°ficos y reportes EDA\n",
    "    - Reconstruye archivos con fechas originales\n",
    "    - Llena NaNs y genera archivos finales\n",
    "    - Genera informes EDA finales con resumen de limpieza\n",
    "    \"\"\"\n",
    "    print(\"Iniciando procesamiento...\")\n",
    "    # Definir los metadatos para cada sitio\n",
    "    metadatos_salvador = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00001\", \"City\": \"Salvador\", \"State\": \"Atacama\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -26.2533, \"Longitude\": -69.0522, \"Time Zone\": -4, \"Elevation\": 2280\n",
    "    }\n",
    "    metadatos_calama = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00002\", \"City\": \"Calama\", \"State\": \"Antofagasta\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -22.4661, \"Longitude\": -68.9244, \"Time Zone\": -4, \"Elevation\": 2260\n",
    "    }\n",
    "    metadatos_vallenar = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00003\", \"City\": \"Vallenar\", \"State\": \"Atacama\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -28.5766, \"Longitude\": -70.7601, \"Time Zone\": -4, \"Elevation\": 441\n",
    "    }\n",
    "    # Rutas de archivos\n",
    "    path_salvador = Path(\"salvador_corrupted.csv\")\n",
    "    path_calama = Path(\"calama_corrupted.csv\")\n",
    "    path_vallenar = Path(\"Vallenar_corrupted.csv\")\n",
    "    # Salidas\n",
    "    output_salvador = Path(\"salvador_TMY_artificial.csv\")\n",
    "    output_calama = Path(\"calama_TMY_artificial.csv\")\n",
    "    output_vallenar = Path(\"vallenar_TMY_artificial.csv\")\n",
    "    output_salvador_limpio = Path(DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\")\n",
    "    output_calama_limpio = Path(DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\")\n",
    "    output_vallenar_limpio = Path(DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\")\n",
    "\n",
    "    # Procesar cada archivo (sin generar el informe EDA aqu√≠)\n",
    "    for location, (input_path, output_path, output_limpio, metadata) in [\n",
    "        (\"Salvador\", (path_salvador, output_salvador, output_salvador_limpio, metadatos_salvador)),\n",
    "        (\"Calama\", (path_calama, output_calama, output_calama_limpio, metadatos_calama)),\n",
    "        (\"Vallenar\", (path_vallenar, output_vallenar, output_vallenar_limpio, metadatos_vallenar))\n",
    "    ]:\n",
    "        try:\n",
    "            print(f\"\\nProcesando {location}...\")\n",
    "            print(f\"Leyendo archivo: {input_path}\")\n",
    "            df = transformar_a_tmy_con_metadatos(input_path, output_path, metadata)\n",
    "            print(f\"Transformaci√≥n completada. Limpiando datos...\")\n",
    "            limpiar_TMY_completo(str(output_path), str(output_limpio), location=location)\n",
    "            print(f\"Limpieza completada. Eliminando archivos temporales...\")\n",
    "            for suffix in [\"_GHI_limpio.csv\", \"_DNI_limpio.csv\", \"_DHI_limpio.csv\"]:\n",
    "                try:\n",
    "                    os.remove(str(output_path).replace(\"_TMY_artificial.csv\", suffix))\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            print(f\"Leyendo archivo limpio...\")\n",
    "            df_limpio = pd.read_csv(output_limpio, skiprows=2)\n",
    "            print(f\"Generando gr√°ficos...\")\n",
    "            plot_tmy_data(df_limpio, location)\n",
    "            print(f\"Realizando an√°lisis estacional...\")\n",
    "            analisis_estacional(df_limpio, location)\n",
    "            print(f\"Eliminando archivo temporal...\")\n",
    "            try:\n",
    "                os.remove(str(output_path))\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            print(f\"Procesamiento de {location} completado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {location}: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    # Generar resumen comparativo\n",
    "    dfs_limpios = [\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\", skiprows=2),\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\", skiprows=2),\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\", skiprows=2)\n",
    "    ]\n",
    "    locations = [\"Salvador\", \"Calama\", \"Vallenar\"]\n",
    "    if len(dfs_limpios) == 3:\n",
    "        print(\"\\nGenerando resumen comparativo...\")\n",
    "        generar_resumen_comparativo(dfs_limpios, locations)\n",
    "        print(\"\\nReconstruyendo archivos TMY limpios con fechas originales...\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"salvador_corrupted.csv\", DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio_originales.csv\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"calama_corrupted.csv\", DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"calama_TMY_limpio_originales.csv\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"Vallenar_corrupted.csv\", DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio_originales.csv\")\n",
    "    else:\n",
    "        print(f\"\\nNo se pudo generar el resumen comparativo. Se procesaron {len(dfs_limpios)} de 3 archivos.\")\n",
    "\n",
    "    # Procesar cada archivo limpio y guardar el resultado final\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'calama_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'calama_TMY_final.csv')\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'salvador_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'salvador_TMY_final.csv')\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'vallenar_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'vallenar_TMY_final.csv')\n",
    "\n",
    "    # Ahora s√≠, generar el informe EDA con ambos DataFrames\n",
    "    for location in [\"Calama\", \"Salvador\", \"Vallenar\"]:\n",
    "        df_limpio = pd.read_csv(DATOS_LIMPIOS_DIR / f\"{location.lower()}_TMY_limpio.csv\", skiprows=2)\n",
    "        df_final = pd.read_csv(DATOS_LIMPIOS_DIR / f\"{location.lower()}_TMY_final.csv\", skiprows=2)\n",
    "        generate_eda_report(df_limpio, location, df_final=df_final)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_tmy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87065a",
   "metadata": {},
   "source": [
    "Los datos fueron limpiados manteniendo la estructura original de registros horarios. La interpolaci√≥n aplicada no introduce discontinuidades notables y mejora la continuidad temporal, permitiendo una entrada confiable a las simulaciones fotovoltaicas. Cada archivo fue validado visualmente para confirmar la correcci√≥n de valores an√≥malos, garantizando que los datasets representan condiciones f√≠sicas razonables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79727a",
   "metadata": {},
   "source": [
    "## 3. Simulaci√≥n de planta PV \n",
    "Se utiliz√≥ el modelo PVWatts v8 de PySAM para simular una planta fotovoltaica de 50 MWDC en cada una de las tres localidades. La configuraci√≥n base fue:\n",
    "\n",
    "- Capacidad: 50 MWDC\n",
    "- Relaci√≥n DC/AC: 1.2\n",
    "- P√©rdidas totales: 14 %\n",
    "- Sistema fijo con seguimiento a 1 eje en algunos casos de prueba\n",
    "\n",
    "Cada simulaci√≥n se ejecut√≥ para los datos meteorol√≥gicos limpios TMY, generando la producci√≥n horaria de energ√≠a AC.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d4c67bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando Calama...\n",
      "\n",
      "An√°lisis de sensibilidad para Calama:\n",
      "Producci√≥n anual base: 79.28 GWh\n",
      "LCOE base: 0.0556 $/kWh\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> LCOE: 0.0506 $/kWh (Impacto: -9.0%)\n",
      "  Variaci√≥n alta: 0.1 -> LCOE: 0.0427 $/kWh (Impacto: -23.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> LCOE: 0.0461 $/kWh (Impacto: -17.1%)\n",
      "  Variaci√≥n alta: 1200 -> LCOE: 0.0578 $/kWh (Impacto: 4.0%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.005 -> LCOE: 0.0440 $/kWh (Impacto: -21.0%)\n",
      "  Variaci√≥n alta: 0.015 -> LCOE: 0.0482 $/kWh (Impacto: -13.3%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> LCOE: 0.0483 $/kWh (Impacto: -13.2%)\n",
      "  Variaci√≥n alta: 12 -> LCOE: 0.0457 $/kWh (Impacto: -17.8%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> LCOE: 0.0452 $/kWh (Impacto: -18.8%)\n",
      "  Variaci√≥n alta: 16.0 -> LCOE: 0.0471 $/kWh (Impacto: -15.3%)\n",
      "VAN para Calama: $6,595,534.24\n",
      "\n",
      "An√°lisis de sensibilidad VAN para Calama:\n",
      "VAN base: $6,595,534.24\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> VAN: $-45,157,200.75 (Impacto: -784.7%)\n",
      "  Variaci√≥n alta: 0.1 -> VAN: $-46,116,166.56 (Impacto: -799.2%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> VAN: $-35,706,735.03 (Impacto: -641.4%)\n",
      "  Variaci√≥n alta: 1200 -> VAN: $-55,706,735.03 (Impacto: -944.6%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.03 -> VAN: $-65,789,076.05 (Impacto: -1097.5%)\n",
      "  Variaci√≥n alta: 0.07 -> VAN: $-39,012,621.35 (Impacto: -691.5%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> VAN: $-49,396,393.67 (Impacto: -848.9%)\n",
      "  Variaci√≥n alta: 12 -> VAN: $-45,045,937.73 (Impacto: -783.0%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> VAN: $-44,928,349.72 (Impacto: -781.2%)\n",
      "  Variaci√≥n alta: 16.0 -> VAN: $-46,485,120.34 (Impacto: -804.8%)\n",
      "\n",
      "An√°lisis de sensibilidad para Calama:\n",
      "Producci√≥n anual base: 79.28 GWh\n",
      "LCOE base: 0.0556 $/kWh\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> LCOE: 0.0506 $/kWh (Impacto: -9.0%)\n",
      "  Variaci√≥n alta: 0.1 -> LCOE: 0.0427 $/kWh (Impacto: -23.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> LCOE: 0.0461 $/kWh (Impacto: -17.1%)\n",
      "  Variaci√≥n alta: 1200 -> LCOE: 0.0578 $/kWh (Impacto: 4.0%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.005 -> LCOE: 0.0440 $/kWh (Impacto: -21.0%)\n",
      "  Variaci√≥n alta: 0.015 -> LCOE: 0.0482 $/kWh (Impacto: -13.3%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> LCOE: 0.0483 $/kWh (Impacto: -13.2%)\n",
      "  Variaci√≥n alta: 12 -> LCOE: 0.0457 $/kWh (Impacto: -17.8%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> LCOE: 0.0452 $/kWh (Impacto: -18.8%)\n",
      "  Variaci√≥n alta: 16.0 -> LCOE: 0.0471 $/kWh (Impacto: -15.3%)\n",
      "\n",
      "Procesando Salvador...\n",
      "\n",
      "An√°lisis de sensibilidad para Salvador:\n",
      "Producci√≥n anual base: 79.55 GWh\n",
      "LCOE base: 0.0555 $/kWh\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> LCOE: 0.0505 $/kWh (Impacto: -9.0%)\n",
      "  Variaci√≥n alta: 0.1 -> LCOE: 0.0425 $/kWh (Impacto: -23.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> LCOE: 0.0460 $/kWh (Impacto: -17.1%)\n",
      "  Variaci√≥n alta: 1200 -> LCOE: 0.0577 $/kWh (Impacto: 4.0%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.005 -> LCOE: 0.0438 $/kWh (Impacto: -21.0%)\n",
      "  Variaci√≥n alta: 0.015 -> LCOE: 0.0481 $/kWh (Impacto: -13.3%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> LCOE: 0.0481 $/kWh (Impacto: -13.2%)\n",
      "  Variaci√≥n alta: 12 -> LCOE: 0.0456 $/kWh (Impacto: -17.8%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> LCOE: 0.0450 $/kWh (Impacto: -18.8%)\n",
      "  Variaci√≥n alta: 16.0 -> LCOE: 0.0470 $/kWh (Impacto: -15.3%)\n",
      "VAN para Salvador: $6,865,763.93\n",
      "\n",
      "An√°lisis de sensibilidad VAN para Salvador:\n",
      "VAN base: $6,865,763.93\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> VAN: $-45,025,219.56 (Impacto: -755.8%)\n",
      "  Variaci√≥n alta: 0.1 -> VAN: $-46,018,203.36 (Impacto: -770.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> VAN: $-35,593,760.34 (Impacto: -618.4%)\n",
      "  Variaci√≥n alta: 1200 -> VAN: $-55,593,760.34 (Impacto: -909.7%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.03 -> VAN: $-65,743,886.18 (Impacto: -1057.6%)\n",
      "  Variaci√≥n alta: 0.07 -> VAN: $-38,877,051.73 (Impacto: -666.2%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> VAN: $-49,283,418.98 (Impacto: -817.8%)\n",
      "  Variaci√≥n alta: 12 -> VAN: $-44,932,963.04 (Impacto: -754.4%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> VAN: $-44,812,747.71 (Impacto: -752.7%)\n",
      "  Variaci√≥n alta: 16.0 -> VAN: $-46,374,772.97 (Impacto: -775.4%)\n",
      "\n",
      "An√°lisis de sensibilidad para Salvador:\n",
      "Producci√≥n anual base: 79.55 GWh\n",
      "LCOE base: 0.0555 $/kWh\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> LCOE: 0.0505 $/kWh (Impacto: -9.0%)\n",
      "  Variaci√≥n alta: 0.1 -> LCOE: 0.0425 $/kWh (Impacto: -23.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> LCOE: 0.0460 $/kWh (Impacto: -17.1%)\n",
      "  Variaci√≥n alta: 1200 -> LCOE: 0.0577 $/kWh (Impacto: 4.0%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.005 -> LCOE: 0.0438 $/kWh (Impacto: -21.0%)\n",
      "  Variaci√≥n alta: 0.015 -> LCOE: 0.0481 $/kWh (Impacto: -13.3%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> LCOE: 0.0481 $/kWh (Impacto: -13.2%)\n",
      "  Variaci√≥n alta: 12 -> LCOE: 0.0456 $/kWh (Impacto: -17.8%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> LCOE: 0.0450 $/kWh (Impacto: -18.8%)\n",
      "  Variaci√≥n alta: 16.0 -> LCOE: 0.0470 $/kWh (Impacto: -15.3%)\n",
      "\n",
      "Procesando Vallenar...\n",
      "\n",
      "An√°lisis de sensibilidad para Vallenar:\n",
      "Producci√≥n anual base: 64.73 GWh\n",
      "LCOE base: 0.0670 $/kWh\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> LCOE: 0.0608 $/kWh (Impacto: -9.3%)\n",
      "  Variaci√≥n alta: 0.1 -> LCOE: 0.0514 $/kWh (Impacto: -23.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> LCOE: 0.0555 $/kWh (Impacto: -17.2%)\n",
      "  Variaci√≥n alta: 1200 -> LCOE: 0.0699 $/kWh (Impacto: 4.2%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.005 -> LCOE: 0.0534 $/kWh (Impacto: -20.4%)\n",
      "  Variaci√≥n alta: 0.015 -> LCOE: 0.0577 $/kWh (Impacto: -14.0%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> LCOE: 0.0582 $/kWh (Impacto: -13.2%)\n",
      "  Variaci√≥n alta: 12 -> LCOE: 0.0550 $/kWh (Impacto: -17.9%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> LCOE: 0.0544 $/kWh (Impacto: -18.9%)\n",
      "  Variaci√≥n alta: 16.0 -> LCOE: 0.0567 $/kWh (Impacto: -15.4%)\n",
      "VAN para Vallenar: $-8,098,071.76\n",
      "\n",
      "An√°lisis de sensibilidad VAN para Vallenar:\n",
      "VAN base: $-8,098,071.76\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> VAN: $-52,333,612.80 (Impacto: -546.2%)\n",
      "  Variaci√≥n alta: 0.1 -> VAN: $-51,442,867.60 (Impacto: -535.2%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> VAN: $-41,849,677.94 (Impacto: -416.8%)\n",
      "  Variaci√≥n alta: 1200 -> VAN: $-61,849,677.94 (Impacto: -663.8%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.03 -> VAN: $-68,246,253.22 (Impacto: -742.7%)\n",
      "  Variaci√≥n alta: 0.07 -> VAN: $-46,384,152.85 (Impacto: -472.8%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> VAN: $-55,539,336.58 (Impacto: -585.8%)\n",
      "  Variaci√≥n alta: 12 -> VAN: $-51,188,880.65 (Impacto: -532.1%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> VAN: $-51,214,151.77 (Impacto: -532.4%)\n",
      "  Variaci√≥n alta: 16.0 -> VAN: $-52,485,204.12 (Impacto: -548.1%)\n",
      "\n",
      "An√°lisis de sensibilidad para Vallenar:\n",
      "Producci√≥n anual base: 64.73 GWh\n",
      "LCOE base: 0.0670 $/kWh\n",
      "\n",
      "FCR:\n",
      "  Variaci√≥n baja: 0.06 -> LCOE: 0.0608 $/kWh (Impacto: -9.3%)\n",
      "  Variaci√≥n alta: 0.1 -> LCOE: 0.0514 $/kWh (Impacto: -23.3%)\n",
      "\n",
      "CapEx PV:\n",
      "  Variaci√≥n baja: 800 -> LCOE: 0.0555 $/kWh (Impacto: -17.2%)\n",
      "  Variaci√≥n alta: 1200 -> LCOE: 0.0699 $/kWh (Impacto: 4.2%)\n",
      "\n",
      "Spot Price:\n",
      "  Variaci√≥n baja: 0.005 -> LCOE: 0.0534 $/kWh (Impacto: -20.4%)\n",
      "  Variaci√≥n alta: 0.015 -> LCOE: 0.0577 $/kWh (Impacto: -14.0%)\n",
      "\n",
      "Inverter Lifetime:\n",
      "  Variaci√≥n baja: 8 -> LCOE: 0.0582 $/kWh (Impacto: -13.2%)\n",
      "  Variaci√≥n alta: 12 -> LCOE: 0.0550 $/kWh (Impacto: -17.9%)\n",
      "\n",
      "System Losses:\n",
      "  Variaci√≥n baja: 12.0 -> LCOE: 0.0544 $/kWh (Impacto: -18.9%)\n",
      "  Variaci√≥n alta: 16.0 -> LCOE: 0.0567 $/kWh (Impacto: -15.4%)\n",
      "Resultados horarios guardados en pv_simulation_results_hourly.csv\n",
      "\n",
      "Resultados de la simulaci√≥n:\n",
      "Location  Annual Energy (GWh)  LCOE ($/kWh)  Incident Energy (kWh/m¬≤)     NPV (VAN)\n",
      "  Calama            79.280263      0.055640               2671.432475  6.595534e+06\n",
      "Salvador            79.547861      0.055469               2621.646200  6.865764e+06\n",
      "Vallenar            64.729745      0.067043               2249.587847 -8.098072e+06\n",
      "\n",
      "Resultados guardados en pv_simulation_results.csv\n",
      "\n",
      "Gr√°fico tornado combinado guardado como 'tornado_analysis_combined.png'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simulaci√≥n de plantas fotovoltaicas\n",
    "------------------------------------------------\n",
    "Este script permite calcular la energ√≠a incidente, simular la producci√≥n de energ√≠a de una planta fotovoltaica,\n",
    "calcular el LCOE y VAN, y realizar an√°lisis de sensibilidad para distintos par√°metros econ√≥micos y t√©cnicos.\n",
    "\n",
    "Requiere archivos TMY de recurso solar para cada localidad y genera resultados y gr√°ficos en la carpeta 'resultados_pv'.\n",
    "\n",
    "Autor: Nicole Torres\n",
    "Fecha: 14/05/2025\n",
    "\"\"\"\n",
    "# =============================\n",
    "# üîß Definici√≥n de funciones\n",
    "# =============================\n",
    "def calculate_incident_energy(solar_resource_file):\n",
    "    \"\"\"\n",
    "    Calcula la energ√≠a incidente anual a partir de datos TMY.\n",
    "    Args:\n",
    "        solar_resource_file (str): Ruta al archivo de recurso solar (TMY)\n",
    "    Returns:\n",
    "        float: Energ√≠a incidente anual en kWh/m¬≤\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(solar_resource_file, skiprows=2, sep=',', on_bad_lines='skip', encoding='utf-8')\n",
    "        ghi_columns = [col for col in df.columns if 'GHI' in col.upper()]\n",
    "        if not ghi_columns:\n",
    "            raise ValueError(f\"No se encontr√≥ columna GHI en {solar_resource_file}\")\n",
    "        ghi_column = ghi_columns[0]\n",
    "        annual_incident_energy = df[ghi_column].sum() / 1000  # W/m¬≤ a kWh/m¬≤\n",
    "        return annual_incident_energy\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo {solar_resource_file}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def calculate_lcoe(annual_energy, system_capacity_kw, fixed_charge_rate=0.08, project_lifetime=25,\n",
    "                  capex_pv=800, fixed_om_cost=50, variable_om_cost=0.01, inverter_lifetime=10,\n",
    "                  system_losses=14.0):\n",
    "    \"\"\"\n",
    "    Calcula el Costo Nivelado de Energ√≠a (LCOE).\n",
    "    Args:\n",
    "        annual_energy (float): Producci√≥n anual de energ√≠a en kWh\n",
    "        system_capacity_kw (float): Capacidad del sistema en kW\n",
    "        fixed_charge_rate (float): Tasa de cargo fijo (por defecto 8%)\n",
    "        project_lifetime (int): Vida √∫til del proyecto en a√±os (por defecto 25)\n",
    "        capex_pv (float): Costo de capital por kW\n",
    "        fixed_om_cost (float): Costo fijo de O&M por kW/a√±o\n",
    "        variable_om_cost (float): Costo variable de O&M por kWh\n",
    "        inverter_lifetime (int): Vida √∫til del inversor en a√±os\n",
    "        system_losses (float): P√©rdidas del sistema en %\n",
    "    Returns:\n",
    "        float: LCOE en $/kWh\n",
    "    \"\"\"\n",
    "    capital_cost = system_capacity_kw * capex_pv\n",
    "    num_replacements = int(project_lifetime / inverter_lifetime) - 1\n",
    "    if num_replacements > 0:\n",
    "        inverter_cost = system_capacity_kw * 200  # $200/kW para reemplazo\n",
    "        for i in range(num_replacements):\n",
    "            replacement_year = (i + 1) * inverter_lifetime\n",
    "            capital_cost += inverter_cost / (1 + fixed_charge_rate)**replacement_year\n",
    "    fixed_operating_cost = system_capacity_kw * fixed_om_cost\n",
    "    variable_operating_cost = variable_om_cost\n",
    "    annual_energy = annual_energy * (1 - system_losses/100)\n",
    "    discount_rate = fixed_charge_rate\n",
    "    pv_factor = (1 - (1 + discount_rate)**-project_lifetime) / discount_rate\n",
    "    total_capital_cost = capital_cost\n",
    "    total_fixed_om_cost = fixed_operating_cost * pv_factor\n",
    "    total_variable_om_cost = variable_operating_cost * annual_energy * pv_factor\n",
    "    total_energy = annual_energy * project_lifetime\n",
    "    lcoe = (total_capital_cost + total_fixed_om_cost + total_variable_om_cost) / total_energy\n",
    "    return lcoe\n",
    "\n",
    "def calculate_npv(\n",
    "    annual_energy, \n",
    "    system_capacity_kw, \n",
    "    spot_price=0.06,  # Precio de venta de energ√≠a ($/kWh)\n",
    "    fixed_charge_rate=0.08, \n",
    "    project_lifetime=25,\n",
    "    capex_pv=800, \n",
    "    fixed_om_cost=50, \n",
    "    variable_om_cost=0.01, \n",
    "    inverter_lifetime=10,\n",
    "    system_losses=14.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Calcula el Valor Actual Neto (VAN/NPV) del proyecto.\n",
    "    Args:\n",
    "        annual_energy (float): Producci√≥n anual de energ√≠a en kWh\n",
    "        system_capacity_kw (float): Capacidad del sistema en kW\n",
    "        spot_price (float): Precio de venta de energ√≠a ($/kWh)\n",
    "        fixed_charge_rate (float): Tasa de descuento\n",
    "        project_lifetime (int): Vida √∫til del proyecto en a√±os\n",
    "        capex_pv (float): Costo de capital por kW\n",
    "        fixed_om_cost (float): Costo fijo de O&M por kW/a√±o\n",
    "        variable_om_cost (float): Costo variable de O&M por kWh\n",
    "        inverter_lifetime (int): Vida √∫til del inversor en a√±os\n",
    "        system_losses (float): P√©rdidas del sistema en %\n",
    "    Returns:\n",
    "        float: VAN en d√≥lares\n",
    "    \"\"\"\n",
    "    capital_cost = system_capacity_kw * capex_pv\n",
    "    npv = -capital_cost\n",
    "    annual_energy = annual_energy * (1 - system_losses / 100)\n",
    "    for year in range(1, project_lifetime + 1):\n",
    "        revenue = annual_energy * spot_price\n",
    "        fixed_om = system_capacity_kw * fixed_om_cost\n",
    "        variable_om = variable_om_cost * annual_energy\n",
    "        inverter_replacement = 0\n",
    "        if inverter_lifetime > 0 and year % inverter_lifetime == 0 and year != project_lifetime:\n",
    "            inverter_replacement = system_capacity_kw * 200\n",
    "        cash_flow = revenue - fixed_om - variable_om - inverter_replacement\n",
    "        npv += cash_flow / (1 + fixed_charge_rate) ** year\n",
    "    return npv\n",
    "\n",
    "def run_sensitivity_analysis(annual_energy, system_capacity_kw, base_lcoe, location_name):\n",
    "    \"\"\"\n",
    "    Realiza an√°lisis de sensibilidad para el LCOE respecto a distintos par√°metros.\n",
    "    Args:\n",
    "        annual_energy (float): Producci√≥n anual base\n",
    "        system_capacity_kw (float): Capacidad del sistema\n",
    "        base_lcoe (float): LCOE base\n",
    "        location_name (str): Nombre de la localidad\n",
    "    Returns:\n",
    "        tuple: (parameters, variations, impacts)\n",
    "    \"\"\"\n",
    "    parameters = ['FCR', 'CapEx PV', 'Spot Price', 'Inverter Lifetime', 'System Losses']\n",
    "    base_params = {\n",
    "        'FCR': 0.08,\n",
    "        'CapEx PV': 1000,\n",
    "        'Spot Price': 0.06,\n",
    "        'Inverter Lifetime': 10,\n",
    "        'System Losses': 14.0\n",
    "    }\n",
    "    variations = {\n",
    "        'FCR': {'base': base_params['FCR'], 'low': 0.06, 'high': 0.10},\n",
    "        'CapEx PV': {'base': base_params['CapEx PV'], 'low': 800, 'high': 1200},\n",
    "        'Spot Price': {'base': base_params['Spot Price'], 'low': 0.005, 'high': 0.015},\n",
    "        'Inverter Lifetime': {'base': base_params['Inverter Lifetime'], 'low': 8, 'high': 12},\n",
    "        'System Losses': {'base': base_params['System Losses'], 'low': 12.0, 'high': 16.0}\n",
    "    }\n",
    "    impacts = []\n",
    "    print(f\"\\nAn√°lisis de sensibilidad para {location_name}:\")\n",
    "    print(f\"Producci√≥n anual base: {annual_energy/1e6:.2f} GWh\")\n",
    "    print(f\"LCOE base: {base_lcoe:.4f} $/kWh\")\n",
    "    for param in parameters:\n",
    "        # Variaci√≥n baja\n",
    "        if param == 'FCR':\n",
    "            lcoe_low = calculate_lcoe(annual_energy, system_capacity_kw, fixed_charge_rate=variations[param]['low'])\n",
    "        elif param == 'CapEx PV':\n",
    "            lcoe_low = calculate_lcoe(annual_energy, system_capacity_kw, capex_pv=variations[param]['low'])\n",
    "        elif param == 'Spot Price':\n",
    "            lcoe_low = calculate_lcoe(annual_energy, system_capacity_kw, variable_om_cost=variations[param]['low'])\n",
    "        elif param == 'Inverter Lifetime':\n",
    "            lcoe_low = calculate_lcoe(annual_energy, system_capacity_kw, inverter_lifetime=variations[param]['low'])\n",
    "        else:\n",
    "            lcoe_low = calculate_lcoe(annual_energy, system_capacity_kw, system_losses=variations[param]['low'])\n",
    "        # Variaci√≥n alta\n",
    "        if param == 'FCR':\n",
    "            lcoe_high = calculate_lcoe(annual_energy, system_capacity_kw, fixed_charge_rate=variations[param]['high'])\n",
    "        elif param == 'CapEx PV':\n",
    "            lcoe_high = calculate_lcoe(annual_energy, system_capacity_kw, capex_pv=variations[param]['high'])\n",
    "        elif param == 'Spot Price':\n",
    "            lcoe_high = calculate_lcoe(annual_energy, system_capacity_kw, variable_om_cost=variations[param]['high'])\n",
    "        elif param == 'Inverter Lifetime':\n",
    "            lcoe_high = calculate_lcoe(annual_energy, system_capacity_kw, inverter_lifetime=variations[param]['high'])\n",
    "        else:\n",
    "            lcoe_high = calculate_lcoe(annual_energy, system_capacity_kw, system_losses=variations[param]['high'])\n",
    "        impact_low = (lcoe_low - base_lcoe) / base_lcoe * 100\n",
    "        impact_high = (lcoe_high - base_lcoe) / base_lcoe * 100\n",
    "        impacts.append((impact_low, impact_high))\n",
    "        print(f\"\\n{param}:\")\n",
    "        print(f\"  Variaci√≥n baja: {variations[param]['low']} -> LCOE: {lcoe_low:.4f} $/kWh (Impacto: {impact_low:.1f}%)\")\n",
    "        print(f\"  Variaci√≥n alta: {variations[param]['high']} -> LCOE: {lcoe_high:.4f} $/kWh (Impacto: {impact_high:.1f}%)\")\n",
    "    return parameters, variations, impacts\n",
    "\n",
    "def run_npv_sensitivity_analysis(\n",
    "    annual_energy, \n",
    "    system_capacity_kw, \n",
    "    base_npv, \n",
    "    location_name,\n",
    "    spot_price=0.12\n",
    "):\n",
    "    \"\"\"\n",
    "    Realiza an√°lisis de sensibilidad sobre el VAN (NPV) para distintos par√°metros.\n",
    "    Args:\n",
    "        annual_energy (float): Producci√≥n anual base\n",
    "        system_capacity_kw (float): Capacidad del sistema\n",
    "        base_npv (float): VAN base\n",
    "        location_name (str): Nombre de la localidad\n",
    "        spot_price (float): Precio de venta de energ√≠a\n",
    "    Returns:\n",
    "        tuple: (parameters, variations, impacts)\n",
    "    \"\"\"\n",
    "    parameters = ['FCR', 'CapEx PV', 'Spot Price', 'Inverter Lifetime', 'System Losses']\n",
    "    base_params = {\n",
    "        'FCR': 0.08,\n",
    "        'CapEx PV': 1000,\n",
    "        'Spot Price': spot_price,\n",
    "        'Inverter Lifetime': 10,\n",
    "        'System Losses': 14.0\n",
    "    }\n",
    "    variations = {\n",
    "        'FCR': {'low': 0.06, 'high': 0.10},\n",
    "        'CapEx PV': {'low': 800, 'high': 1200},\n",
    "        'Spot Price': {'low': 0.03, 'high': 0.07},\n",
    "        'Inverter Lifetime': {'low': 8, 'high': 12},\n",
    "        'System Losses': {'low': 12.0, 'high': 16.0}\n",
    "    }\n",
    "    impacts = []\n",
    "    print(f\"\\nAn√°lisis de sensibilidad VAN para {location_name}:\")\n",
    "    print(f\"VAN base: ${base_npv:,.2f}\")\n",
    "    for param in parameters:\n",
    "        # Variaci√≥n baja\n",
    "        kwargs = dict(\n",
    "            annual_energy=annual_energy,\n",
    "            system_capacity_kw=system_capacity_kw,\n",
    "            spot_price=base_params['Spot Price'],\n",
    "            fixed_charge_rate=base_params['FCR'],\n",
    "            project_lifetime=20,\n",
    "            capex_pv=base_params['CapEx PV'],\n",
    "            fixed_om_cost=50,\n",
    "            variable_om_cost=0.01,\n",
    "            inverter_lifetime=base_params['Inverter Lifetime'],\n",
    "            system_losses=base_params['System Losses']\n",
    "        )\n",
    "        if param == 'FCR':\n",
    "            kwargs['fixed_charge_rate'] = variations[param]['low']\n",
    "        elif param == 'CapEx PV':\n",
    "            kwargs['capex_pv'] = variations[param]['low']\n",
    "        elif param == 'Spot Price':\n",
    "            kwargs['spot_price'] = variations[param]['low']\n",
    "        elif param == 'Inverter Lifetime':\n",
    "            kwargs['inverter_lifetime'] = variations[param]['low']\n",
    "        elif param == 'System Losses':\n",
    "            kwargs['system_losses'] = variations[param]['low']\n",
    "        npv_low = calculate_npv(**kwargs)\n",
    "        # Variaci√≥n alta\n",
    "        kwargs = dict(\n",
    "            annual_energy=annual_energy,\n",
    "            system_capacity_kw=system_capacity_kw,\n",
    "            spot_price=base_params['Spot Price'],\n",
    "            fixed_charge_rate=base_params['FCR'],\n",
    "            project_lifetime=20,\n",
    "            capex_pv=base_params['CapEx PV'],\n",
    "            fixed_om_cost=50,\n",
    "            variable_om_cost=0.01,\n",
    "            inverter_lifetime=base_params['Inverter Lifetime'],\n",
    "            system_losses=base_params['System Losses']\n",
    "        )\n",
    "        if param == 'FCR':\n",
    "            kwargs['fixed_charge_rate'] = variations[param]['high']\n",
    "        elif param == 'CapEx PV':\n",
    "            kwargs['capex_pv'] = variations[param]['high']\n",
    "        elif param == 'Spot Price':\n",
    "            kwargs['spot_price'] = variations[param]['high']\n",
    "        elif param == 'Inverter Lifetime':\n",
    "            kwargs['inverter_lifetime'] = variations[param]['high']\n",
    "        elif param == 'System Losses':\n",
    "            kwargs['system_losses'] = variations[param]['high']\n",
    "        npv_high = calculate_npv(**kwargs)\n",
    "        impact_low = (npv_low - base_npv) / abs(base_npv) * 100\n",
    "        impact_high = (npv_high - base_npv) / abs(base_npv) * 100\n",
    "        impacts.append((impact_low, impact_high))\n",
    "        print(f\"\\n{param}:\")\n",
    "        print(f\"  Variaci√≥n baja: {variations[param]['low']} -> VAN: ${npv_low:,.2f} (Impacto: {impact_low:.1f}%)\")\n",
    "        print(f\"  Variaci√≥n alta: {variations[param]['high']} -> VAN: ${npv_high:,.2f} (Impacto: {impact_high:.1f}%)\")\n",
    "    return parameters, variations, impacts\n",
    "\n",
    "def plot_tornado(parameters, impacts, location_name):\n",
    "    \"\"\"\n",
    "    Genera un gr√°fico tornado para el an√°lisis de sensibilidad del LCOE.\n",
    "    Args:\n",
    "        parameters (list): Lista de nombres de par√°metros\n",
    "        impacts (list): Lista de tuplas (bajo, alto) de impacto\n",
    "        location_name (str): Nombre de la localidad\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort([abs(high - low) for low, high in impacts])\n",
    "    sorted_params = [parameters[i] for i in sorted_indices]\n",
    "    sorted_impacts = [impacts[i] for i in sorted_indices]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    y_pos = np.arange(len(sorted_params))\n",
    "    width = 0.35\n",
    "    ax.barh(y_pos - width/2, [low for low, _ in sorted_impacts], width, color='red', label='Variaci√≥n baja')\n",
    "    ax.barh(y_pos + width/2, [high for _, high in sorted_impacts], width, color='green', label='Variaci√≥n alta')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(sorted_params)\n",
    "    ax.set_xlabel('Impacto en LCOE (%)')\n",
    "    ax.set_title(f'An√°lisis de sensibilidad LCOE - {location_name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='x')\n",
    "    for i, (low, high) in enumerate(sorted_impacts):\n",
    "        ax.text(low, i - width/2, f'{low:.1f}%', ha='right', va='center')\n",
    "        ax.text(high, i + width/2, f'{high:.1f}%', ha='left', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTADOS_PV_DIR / f'tornado_analysis_{location_name.lower()}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_npv_tornado(parameters, impacts, location_name):\n",
    "    \"\"\"\n",
    "    Genera un gr√°fico tornado para el an√°lisis de sensibilidad del VAN.\n",
    "    Args:\n",
    "        parameters (list): Lista de nombres de par√°metros\n",
    "        impacts (list): Lista de tuplas (bajo, alto) de impacto\n",
    "        location_name (str): Nombre de la localidad\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort([abs(high - low) for low, high in impacts])\n",
    "    sorted_params = [parameters[i] for i in sorted_indices]\n",
    "    sorted_impacts = [impacts[i] for i in sorted_indices]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    y_pos = np.arange(len(sorted_params))\n",
    "    width = 0.35\n",
    "    ax.barh(y_pos - width/2, [low for low, _ in sorted_impacts], width, color='red', label='Variaci√≥n baja')\n",
    "    ax.barh(y_pos + width/2, [high for _, high in sorted_impacts], width, color='green', label='Variaci√≥n alta')\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(sorted_params)\n",
    "    ax.set_xlabel('Impacto en VAN (%)')\n",
    "    ax.set_title(f'An√°lisis de sensibilidad VAN - {location_name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='x')\n",
    "    for i, (low, high) in enumerate(sorted_impacts):\n",
    "        ax.text(low, i - width/2, f'{low:.1f}%', ha='right', va='center')\n",
    "        ax.text(high, i + width/2, f'{high:.1f}%', ha='left', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTADOS_PV_DIR / f'tornado_npv_{location_name.lower()}.png')\n",
    "    plt.close()\n",
    "\n",
    "def simulate_pv_plant(solar_resource_file, system_capacity_kw, location_name):\n",
    "    \"\"\"\n",
    "    Simula una planta FV usando el modelo PVWatts.\n",
    "    Args:\n",
    "        solar_resource_file (str): Ruta al archivo de recurso solar\n",
    "        system_capacity_kw (float): Capacidad del sistema en kW\n",
    "        location_name (str): Nombre de la localidad\n",
    "    Returns:\n",
    "        tuple: (annual_energy, lcoe, incident_energy, hourly_power)\n",
    "    \"\"\"\n",
    "    incident_energy = calculate_incident_energy(solar_resource_file)\n",
    "    if incident_energy is None:\n",
    "        print(f\"Advertencia: No se pudo calcular la energ√≠a incidente para {location_name}\")\n",
    "        incident_energy = 0\n",
    "    pv_model = pv.new()\n",
    "    pv_model.SolarResource.solar_resource_file = solar_resource_file\n",
    "    pv_model.SystemDesign.system_capacity = system_capacity_kw\n",
    "    pv_model.SystemDesign.dc_ac_ratio = 1.2\n",
    "    pv_model.SystemDesign.array_type = 1  # Fijo\n",
    "    pv_model.SystemDesign.azimuth = 180   # Sur\n",
    "    pv_model.SystemDesign.tilt = 20       # Inclinaci√≥n 20¬∞\n",
    "    pv_model.SystemDesign.gcr = 0.4       # Relaci√≥n cobertura suelo\n",
    "    pv_model.SystemDesign.inv_eff = 96    # Eficiencia inversor\n",
    "    pv_model.SystemDesign.losses = 14.0   # P√©rdidas\n",
    "    pv_model.execute()\n",
    "    annual_energy = pv_model.Outputs.annual_energy \n",
    "    hourly_power = np.array(pv_model.Outputs.ac) / 1000\n",
    "    base_lcoe = calculate_lcoe(annual_energy, system_capacity_kw, fixed_charge_rate=0.08, project_lifetime=20)\n",
    "    parameters, variations, impacts = run_sensitivity_analysis(annual_energy, system_capacity_kw, base_lcoe, location_name)\n",
    "    plot_tornado(parameters, impacts, location_name)\n",
    "    return annual_energy, base_lcoe, incident_energy, hourly_power\n",
    "\n",
    "def plot_combined_tornado(all_results):\n",
    "    \"\"\"\n",
    "    Genera un gr√°fico tornado combinado para todas las localidades.\n",
    "    Args:\n",
    "        all_results (dict): Resultados de cada localidad\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    parameters = ['FCR', 'CapEx PV', 'Spot Price', 'Inverter Lifetime', 'System Losses']\n",
    "    y_pos = np.arange(len(parameters))\n",
    "    width = 0.25\n",
    "    colors = {\n",
    "        'Calama': 'red',\n",
    "        'Salvador': 'green',\n",
    "        'Vallenar': 'blue'\n",
    "    }\n",
    "    for i, location in enumerate(all_results.keys()):\n",
    "        impacts = all_results[location]['impacts']\n",
    "        sorted_indices = np.argsort([abs(high - low) for low, high in impacts])\n",
    "        sorted_impacts = [impacts[i] for i in sorted_indices]\n",
    "        offset = (i - 1) * width\n",
    "        low_bars = ax.barh(y_pos + offset, [low for low, _ in sorted_impacts], width, color=colors[location], alpha=0.6, label=f'{location} (Baja)')\n",
    "        high_bars = ax.barh(y_pos + offset, [high for _, high in sorted_impacts], width, color=colors[location], alpha=0.3, label=f'{location} (Alta)')\n",
    "        for j, bar in enumerate(low_bars):\n",
    "            width_bar = bar.get_width()\n",
    "            ax.text(width_bar, bar.get_y() + bar.get_height()/2, f'{width_bar:.1f}%', ha='right', va='center', fontsize=8)\n",
    "        for j, bar in enumerate(high_bars):\n",
    "            width_bar = bar.get_width()\n",
    "            ax.text(width_bar, bar.get_y() + bar.get_height()/2, f'{width_bar:.1f}%', ha='left', va='center', fontsize=8)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(parameters)\n",
    "    ax.set_xlabel('Impacto en LCOE (%)')\n",
    "    ax.set_title('An√°lisis de sensibilidad LCOE - Todas las localidades')\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "    ax.grid(True, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTADOS_PV_DIR / 'tornado_analysis_combined.png', bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# =============================\n",
    "# üöÄ Ejecuci√≥n principal\n",
    "# =============================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal: ejecuta la simulaci√≥n, an√°lisis de sensibilidad y guarda resultados y gr√°ficos.\n",
    "    \"\"\"\n",
    "    sensibilidad_lcoe = []\n",
    "    sensibilidad_npv = []\n",
    "    system_capacity_kw = 50000  # 50 MW\n",
    "    locations = [\n",
    "        {\"name\": \"Calama\", \"solar_resource\": \"/home/nicole/UA/prueba2/datos_limpios/calama_TMY_final.csv\"},\n",
    "        {\"name\": \"Salvador\", \"solar_resource\": \"/home/nicole/UA/prueba2/datos_limpios/salvador_TMY_final.csv\"},\n",
    "        {\"name\": \"Vallenar\", \"solar_resource\": \"/home/nicole/UA/prueba2/datos_limpios/vallenar_TMY_final.csv\"}\n",
    "    ]\n",
    "    results = []\n",
    "    all_results = {}\n",
    "    hourly_results = []\n",
    "    for loc in locations:\n",
    "        print(f\"\\nProcesando {loc['name']}...\")\n",
    "        annual_energy, lcoe, incident_energy, hourly_power = simulate_pv_plant(\n",
    "            loc[\"solar_resource\"],\n",
    "            system_capacity_kw,\n",
    "            loc[\"name\"]\n",
    "        )\n",
    "        df_tmy = pd.read_csv(loc[\"solar_resource\"], skiprows=2)\n",
    "        datetimes = pd.to_datetime(df_tmy[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n",
    "        df_hourly = pd.DataFrame({\n",
    "            \"datetime\": datetimes,\n",
    "            \"Location\": loc[\"name\"],\n",
    "            \"AC Power (kW)\": hourly_power\n",
    "        })\n",
    "        hourly_results.append(df_hourly)\n",
    "        npv = calculate_npv(\n",
    "            annual_energy, \n",
    "            system_capacity_kw, \n",
    "            spot_price=0.12,  # Cambia este valor si tienes un precio de venta diferente\n",
    "            fixed_charge_rate=0.08,\n",
    "            project_lifetime=25,\n",
    "            capex_pv=800,\n",
    "            fixed_om_cost=50,\n",
    "            variable_om_cost=0.01,\n",
    "            inverter_lifetime=10,\n",
    "            system_losses=14.0\n",
    "        )\n",
    "        print(f\"VAN para {loc['name']}: ${npv:,.2f}\")\n",
    "        parameters_npv, variations_npv, impacts_npv = run_npv_sensitivity_analysis(\n",
    "            annual_energy, system_capacity_kw, npv, loc[\"name\"], spot_price=0.06\n",
    "        )\n",
    "        plot_npv_tornado(parameters_npv, impacts_npv, loc[\"name\"])\n",
    "        for param, (impact_low, impact_high) in zip(parameters_npv, impacts_npv):\n",
    "            sensibilidad_npv.append({\n",
    "                \"Location\": loc[\"name\"],\n",
    "                \"Parameter\": param,\n",
    "                \"Impact Low (%)\": impact_low,\n",
    "                \"Impact High (%)\": impact_high\n",
    "        })\n",
    "        parameters, variations, impacts = run_sensitivity_analysis(\n",
    "            annual_energy, system_capacity_kw, lcoe, loc[\"name\"]\n",
    "        )\n",
    "        for param, (impact_low, impact_high) in zip(parameters, impacts):\n",
    "            sensibilidad_lcoe.append({\n",
    "                \"Location\": loc[\"name\"],\n",
    "                \"Parameter\": param,\n",
    "                \"Impact Low (%)\": impact_low,\n",
    "                \"Impact High (%)\": impact_high\n",
    "        })\n",
    "        all_results[loc[\"name\"]] = {\n",
    "            \"annual_energy\": annual_energy,\n",
    "            \"lcoe\": lcoe,\n",
    "            \"impacts\": impacts\n",
    "        }\n",
    "        results.append({\n",
    "            \"Location\": loc[\"name\"],\n",
    "            \"Annual Energy (GWh)\": annual_energy / 1e6,\n",
    "            \"LCOE ($/kWh)\": lcoe,\n",
    "            \"Incident Energy (kWh/m¬≤)\": incident_energy,\n",
    "            \"NPV (VAN)\": npv\n",
    "        })\n",
    "    df_hourly_all = pd.concat(hourly_results, ignore_index=True)\n",
    "    df_hourly_all.to_csv(RESULTADOS_PV_DIR / \"pv_simulation_results_hourly.csv\", index=False)\n",
    "    print(\"Resultados horarios guardados en pv_simulation_results_hourly.csv\")\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\nResultados de la simulaci√≥n:\")\n",
    "    print(df_results.to_string(index=False))\n",
    "    df_results.to_csv(RESULTADOS_PV_DIR / \"pv_simulation_results.csv\", index=False)\n",
    "    print(\"\\nResultados guardados en pv_simulation_results.csv\")\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    ax1.bar(df_results[\"Location\"], df_results[\"Annual Energy (GWh)\"])\n",
    "    ax1.set_xlabel(\"Localidad\")\n",
    "    ax1.set_ylabel(\"Energ√≠a anual (GWh)\")\n",
    "    ax1.set_title(\"Producci√≥n anual de energ√≠a por localidad\")\n",
    "    ax1.grid(True)\n",
    "    ax2.bar(df_results[\"Location\"], df_results[\"LCOE ($/kWh)\"])\n",
    "    ax2.set_xlabel(\"Localidad\")\n",
    "    ax2.set_ylabel(\"LCOE ($/kWh)\")\n",
    "    ax2.set_title(\"Costo nivelado de energ√≠a (20 a√±os, 8% FCR)\")\n",
    "    ax2.grid(True)\n",
    "    ax3.bar(df_results[\"Location\"], df_results[\"Incident Energy (kWh/m¬≤)\"])\n",
    "    ax3.set_xlabel(\"Localidad\")\n",
    "    ax3.set_ylabel(\"Energ√≠a incidente (kWh/m¬≤)\")\n",
    "    ax3.set_title(\"Energ√≠a incidente anual por localidad\")\n",
    "    ax3.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTADOS_PV_DIR / \"pv_simulation_results.png\")\n",
    "    plt.close()\n",
    "    plot_combined_tornado(all_results)\n",
    "    pd.DataFrame(sensibilidad_lcoe).to_csv(RESULTADOS_PV_DIR / \"sensibilidad_lcoe.csv\", index=False)\n",
    "    pd.DataFrame(sensibilidad_npv).to_csv(RESULTADOS_PV_DIR / \"sensibilidad_npv.csv\", index=False)\n",
    "    print(\"\\nGr√°fico tornado combinado guardado como 'tornado_analysis_combined.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9932006",
   "metadata": {},
   "source": [
    "La simulaci√≥n fue ejecutada exitosamente para cada ubicaci√≥n. Se utilizaron los datos meteorol√≥gicos limpios y se obtuvieron resultados horarios consistentes. La energ√≠a anual obtenida para cada localidad se utilizar√° en las siguientes secciones para el c√°lculo de KPIs econ√≥micos como LCOE y VAN.\n",
    "Cabe destacar que Vallenar entreg√≥ un VAN muy negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfef04",
   "metadata": {},
   "source": [
    "## 3. Dashboard Interactivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, dcc, html, dash_table, Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Carpetas\n",
    "DATOS_LIMPIOS_DIR = Path(\"datos_limpios\")\n",
    "RESULTADOS_PV_DIR = Path(\"resultados_pv\")\n",
    "\n",
    "# Carga datos horarios y generales\n",
    "df_hourly = pd.read_csv(RESULTADOS_PV_DIR / \"pv_simulation_results_hourly.csv\", parse_dates=[\"datetime\"])\n",
    "df_kpi = pd.read_csv(RESULTADOS_PV_DIR / \"pv_simulation_results.csv\")\n",
    "df_sens_lcoe = pd.read_csv(RESULTADOS_PV_DIR / \"sensibilidad_lcoe.csv\")\n",
    "df_sens_npv = pd.read_csv(RESULTADOS_PV_DIR / \"sensibilidad_npv.csv\")\n",
    "\n",
    "# Si no tienes columna Year, extr√°ela del datetime\n",
    "if \"Year\" not in df_hourly.columns:\n",
    "    df_hourly[\"Year\"] = df_hourly[\"datetime\"].dt.year\n",
    "\n",
    "localidades = df_hourly[\"Location\"].unique()\n",
    "anios = df_hourly[\"Year\"].unique()\n",
    "\n",
    "app = Dash(__name__)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Dashboard Solar Integrado\"),\n",
    "    html.Div([\n",
    "        html.Label(\"Selecciona pa√≠s/localidad:\"),\n",
    "        dcc.Dropdown(\n",
    "            id=\"dropdown-localidad\",\n",
    "            options=[{\"label\": loc, \"value\": loc} for loc in localidades],\n",
    "            value=localidades[0]\n",
    "        ),\n",
    "        html.Label(\"Selecciona a√±o:\"),\n",
    "        dcc.Dropdown(\n",
    "            id=\"dropdown-anio\",\n",
    "            options=[{\"label\": str(a), \"value\": a} for a in anios],\n",
    "            value=anios[0]\n",
    "        ),\n",
    "    ], style={\"display\": \"flex\", \"gap\": \"2em\"}),\n",
    "    html.H2(\"Curva horaria de potencia (AC Power)\"),\n",
    "    dcc.Graph(id=\"grafico-potencia\"),\n",
    "    html.H2(\"KPIs diarios\"),\n",
    "    html.Div(id=\"kpi-diarios\", style={\"display\": \"flex\", \"gap\": \"3em\"}),\n",
    "    html.H2(\"Datos TMY limpios (primeras filas)\"),\n",
    "    dash_table.DataTable(id=\"tabla-tmy\", page_size=10, style_table={'overflowX': 'auto'}),\n",
    "    html.H2(\"Gr√°ficos de radiaci√≥n horaria\"),\n",
    "    dcc.Graph(id=\"grafico-tmy\"),\n",
    "    html.H2(\"Resultados de Simulaci√≥n PV\"),\n",
    "    dash_table.DataTable(id=\"tabla-resultados\", page_size=5, style_table={'overflowX': 'auto'}),\n",
    "    html.H2(\"An√°lisis de Sensibilidad LCOE\"),\n",
    "    dcc.Graph(id=\"grafico-tornado-lcoe\"),\n",
    "    dash_table.DataTable(id=\"tabla-sens-lcoe\", page_size=10, style_table={'overflowX': 'auto'}),\n",
    "    html.H2(\"An√°lisis de Sensibilidad NPV\"),\n",
    "    dcc.Graph(id=\"grafico-tornado-npv\"),\n",
    "    dash_table.DataTable(id=\"tabla-sens-npv\", page_size=10, style_table={'overflowX': 'auto'}),\n",
    "])\n",
    "def crear_tornado(df, parametro_impacto_bajo, parametro_impacto_alto, titulo):\n",
    "    # Ordenar por el mayor impacto absoluto\n",
    "    df = df.copy()\n",
    "    df['max_impact'] = df[[parametro_impacto_bajo, parametro_impacto_alto]].abs().max(axis=1)\n",
    "    df = df.sort_values('max_impact', ascending=True)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df['Parameter'],\n",
    "        x=df[parametro_impacto_bajo],\n",
    "        orientation='h',\n",
    "        name='Impacto bajo',\n",
    "        marker_color='steelblue'\n",
    "    ))\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df['Parameter'],\n",
    "        x=df[parametro_impacto_alto],\n",
    "        orientation='h',\n",
    "        name='Impacto alto',\n",
    "        marker_color='indianred'\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        barmode='overlay',\n",
    "        title=titulo,\n",
    "        xaxis_title='Impacto (%)',\n",
    "        yaxis_title='Par√°metro',\n",
    "        template='plotly_white',\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    return fig\n",
    "@app.callback(\n",
    "    Output(\"grafico-potencia\", \"figure\"),\n",
    "    Output(\"kpi-diarios\", \"children\"),\n",
    "    Output(\"tabla-tmy\", \"data\"),\n",
    "    Output(\"tabla-tmy\", \"columns\"),\n",
    "    Output(\"grafico-tmy\", \"figure\"),\n",
    "    Output(\"tabla-resultados\", \"data\"),\n",
    "    Output(\"tabla-resultados\", \"columns\"),\n",
    "    Output(\"grafico-tornado-lcoe\", \"figure\"),\n",
    "    Output(\"tabla-sens-lcoe\", \"data\"),\n",
    "    Output(\"tabla-sens-lcoe\", \"columns\"),\n",
    "    Output(\"grafico-tornado-npv\", \"figure\"),\n",
    "    Output(\"tabla-sens-npv\", \"data\"),\n",
    "    Output(\"tabla-sens-npv\", \"columns\"),\n",
    "    Input(\"dropdown-localidad\", \"value\"),\n",
    "    Input(\"dropdown-anio\", \"value\")\n",
    ")\n",
    "def actualizar_dashboard(localidad, anio):\n",
    "    # --- Potencia horaria y KPIs ---\n",
    "    df_sel = df_hourly[(df_hourly[\"Location\"] == localidad) & (df_hourly[\"Year\"] == anio)].copy()\n",
    "    fig_pot = px.line(df_sel, x=\"datetime\", y=\"AC Power (kW)\", title=f\"Potencia horaria - {localidad} {anio}\")\n",
    "    df_sel[\"date\"] = df_sel[\"datetime\"].dt.date\n",
    "    energia_diaria = df_sel.groupby(\"date\")[\"AC Power (kW)\"].sum()\n",
    "    energia_prom = energia_diaria.mean()\n",
    "    energia_total = energia_diaria.sum()\n",
    "    lcoe = df_kpi[df_kpi[\"Location\"] == localidad][\"LCOE ($/kWh)\"].values[0]\n",
    "    potencia_nominal = df_sel[\"AC Power (kW)\"].max()\n",
    "    cf_diario = energia_diaria / (potencia_nominal * 24)\n",
    "    cf_prom = cf_diario.mean()\n",
    "    kpis = [\n",
    "        html.Div([\n",
    "            html.H3(\"Energ√≠a diaria promedio\"),\n",
    "            html.P(f\"{energia_prom:.2f} kWh/d√≠a\")\n",
    "        ]),\n",
    "        html.Div([\n",
    "            html.H3(\"LCOE\"),\n",
    "            html.P(f\"{lcoe:.4f} $/kWh\")\n",
    "        ]),\n",
    "        html.Div([\n",
    "            html.H3(\"Factor de Capacidad promedio\"),\n",
    "            html.P(f\"{cf_prom*100:.2f} %\")\n",
    "        ]),\n",
    "    ]\n",
    "    # --- TMY limpio y radiaci√≥n ---\n",
    "    tmy_file = DATOS_LIMPIOS_DIR / f\"{localidad.lower()}_TMY_final.csv\"\n",
    "    df_tmy = pd.read_csv(tmy_file, skiprows=2)\n",
    "    tmy_data = df_tmy.head(20).to_dict(\"records\")\n",
    "    tmy_columns = [{\"name\": i, \"id\": i} for i in df_tmy.columns]\n",
    "    fig_tmy = px.line(df_tmy, x=range(len(df_tmy)), y=[\"GHI\", \"DNI\", \"DHI\"], labels={\"value\": \"W/m¬≤\", \"variable\": \"Componente\"}, title=f\"Radiaci√≥n horaria - {localidad}\")\n",
    "    # --- Resultados PV ---\n",
    "    df_res = df_kpi[df_kpi[\"Location\"] == localidad]\n",
    "    res_data = df_res.to_dict(\"records\")\n",
    "    res_columns = [{\"name\": i, \"id\": i} for i in df_res.columns]\n",
    "    # --- Sensibilidad LCOE ---\n",
    "    df_lcoe = df_sens_lcoe[df_sens_lcoe[\"Location\"] == localidad]\n",
    "    fig_lcoe = crear_tornado(df_lcoe, \"Impact Low (%)\", \"Impact High (%)\", f\"Tornado LCOE - {localidad}\")\n",
    "    lcoe_data = df_lcoe.to_dict(\"records\")\n",
    "    lcoe_columns = [{\"name\": i, \"id\": i} for i in df_lcoe.columns]\n",
    "    # --- Sensibilidad NPV ---\n",
    "    df_npv = df_sens_npv[df_sens_npv[\"Location\"] == localidad]\n",
    "    fig_npv = crear_tornado(df_npv, \"Impact Low (%)\", \"Impact High (%)\", f\"Tornado NPV - {localidad}\")    \n",
    "    npv_data = df_npv.to_dict(\"records\")\n",
    "    npv_columns = [{\"name\": i, \"id\": i} for i in df_npv.columns]\n",
    "    return (fig_pot, kpis, tmy_data, tmy_columns, fig_tmy, res_data, res_columns, fig_lcoe, lcoe_data, lcoe_columns, fig_npv, npv_data, npv_columns)\n",
    "\n",
    "app.run(mode='inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones\n",
    "- Salvador tiene el mejor desempe√±o econ√≥mico (mayor VAN, bajo LCOE).\n",
    "- Calama tambi√©n es competitivo, con potencial de mejora mediante reducci√≥n de CapEx o p√©rdidas.\n",
    "- Vallenar no es viable bajo las condiciones actuales, con VAN negativo y bajo rendimiento.\n",
    "- El an√°lisis de sensibilidad muestra fuerte dependencia del modelo econ√≥mico al precio spot y la inversi√≥n.\n",
    "- El dashboard permite explorar din√°micamente los resultados por pa√≠s y a√±o."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
