{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73376cfb",
   "metadata": {},
   "source": [
    "# üåû An√°lisis Exploratorio y Limpieza de Archivos TMY\n",
    "Este notebook transforma, limpia y analiza archivos de datos horarios solares (GHI, DNI y DHI) para Calama, Salvador y Vallenar. Se realiza:\n",
    "\n",
    "- Transformaci√≥n de datos horarios a formato TMY artificial\n",
    "- Limpieza avanzada de GHI, DNI y DHI incluyendo validaci√≥n f√≠sica\n",
    "- Generaci√≥n de reportes EDA y gr√°ficos\n",
    "- Reconstrucci√≥n de archivos limpios con fechas originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a855a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Carga de librer√≠as y definici√≥n de funciones\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pvlib\n",
    "import os\n",
    "from pvlib.solarposition import get_solarposition\n",
    "import PySAM.Pvwattsv7 as pv\n",
    "import PySAM.Lcoefcr as Lcoefcr\n",
    "\n",
    "DATOS_LIMPIOS_DIR = Path('datos_limpios')\n",
    "DATOS_LIMPIOS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98325773",
   "metadata": {},
   "source": [
    "## ‚ñ∂Ô∏è Ejecutar la limpieza y calidad de datos para las tres localidades\n",
    "Ejecuta la funci√≥n `main_tmy()` para procesar los tres archivos TMY, realizar EDA, Limpieza, verificaci√≥n gr√°fica de datos, armado de TMY, guardar resultados y generar gr√°ficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92f1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "def transformar_a_tmy_con_metadatos(csv_path, output_path, metadata_dict):\n",
    "    \"\"\"\n",
    "    Transforma un archivo CSV con datos horarios en un TMY artificial con metadatos en el encabezado.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Eliminar columna 'datetime' si existe\n",
    "    if 'datetime' in df.columns:\n",
    "        df = df.drop(columns=['datetime'])\n",
    "\n",
    "    # Ordenar cronol√≥gicamente y recortar o completar a 8760 filas\n",
    "    df = df.sort_values(by=[\"Month\", \"Day\", \"Hour\", \"Minute\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.iloc[:8760]  # en caso de que tenga m√°s filas\n",
    "\n",
    "    # Asignar un a√±o artificial constante\n",
    "    df['Year'] = 1990\n",
    "\n",
    "    # Reordenar columnas: primero las de fecha\n",
    "    columnas_fecha = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    otras = [c for c in df.columns if c not in columnas_fecha]\n",
    "    df = df[columnas_fecha + otras]\n",
    "\n",
    "    # Escribir archivo con las tres primeras l√≠neas de metadatos\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # L√≠nea 1: encabezados de metadatos\n",
    "        f.write(\"Source,Location ID,City,State,Country,Latitude,Longitude,Time Zone,Elevation\\n\")\n",
    "        # L√≠nea 2: valores de metadatos\n",
    "        f.write(\",\".join(str(metadata_dict[k]) for k in [\n",
    "            \"Source\", \"Location ID\", \"City\", \"State\", \"Country\",\n",
    "            \"Latitude\", \"Longitude\", \"Time Zone\", \"Elevation\"\n",
    "        ]) + \"\\n\")\n",
    "        # L√≠nea 3: encabezado de columnas de datos\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        # Resto de datos\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_tmy_data(df, location, output_dir=\"plots\", is_clean=False):\n",
    "    \"\"\"\n",
    "    Genera gr√°ficos de los datos TMY artificiales.\n",
    "    \"\"\"\n",
    "    # Crear directorio para gr√°ficos si no existe\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Configurar estilo de gr√°ficos\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Crear figura con tres subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))\n",
    "    title_suffix = \" (Datos Limpios)\" if is_clean else \"\"\n",
    "    fig.suptitle(f'Radiaci√≥n Solar por Hora - {location}{title_suffix}', fontsize=16, y=0.95)\n",
    "\n",
    "    # Crear fechas para el eje x\n",
    "    dates = [datetime(1990, int(row['Month']), int(row['Day']), int(row['Hour'])) \n",
    "             for _, row in df.iterrows()]\n",
    "\n",
    "    # Graficar cada componente en su propio subplot\n",
    "    ax1.plot(dates, df['GHI'], 'b-', linewidth=1, alpha=0.7)\n",
    "    ax1.set_title('Radiaci√≥n Global Horizontal (GHI)', fontsize=12)\n",
    "    ax1.set_ylabel('GHI (W/m¬≤)', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1300)  # Establecer l√≠mite superior para GHI\n",
    "\n",
    "    ax2.plot(dates, df['DNI'], 'r-', linewidth=1, alpha=0.7)\n",
    "    ax2.set_title('Radiaci√≥n Normal Directa (DNI)', fontsize=12)\n",
    "    ax2.set_ylabel('DNI (W/m¬≤)', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1300)  # Establecer l√≠mite superior para DNI\n",
    "\n",
    "    ax3.plot(dates, df['DHI'], 'g-', linewidth=1, alpha=0.7)\n",
    "    ax3.set_title('Radiaci√≥n Horizontal Difusa (DHI)', fontsize=12)\n",
    "    ax3.set_ylabel('DHI (W/m¬≤)', fontsize=10)\n",
    "    ax3.set_xlabel('Mes', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 600)  # Establecer l√≠mite superior para DHI\n",
    "\n",
    "    # Configurar formato del eje x para todos los subplots\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Ajustar layout y guardar\n",
    "    plt.tight_layout()\n",
    "    suffix = \"_clean\" if is_clean else \"\"\n",
    "    plt.savefig(f'{output_dir}/{location.lower()}_tmy_plots{suffix}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_eda_report(df, location, output_dir=\"reports\", df_final=None):\n",
    "    \"\"\"\n",
    "    Genera un informe EDA con an√°lisis de valores faltantes y outliers.\n",
    "    \"\"\"\n",
    "    # Crear directorio para informes si no existe\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Crear archivo de informe\n",
    "    report_path = Path(output_dir) / f\"{location.lower()}_eda_report.txt\"\n",
    "    print(f\"[DEBUG] Generando reporte EDA en: {report_path}\")\n",
    "\n",
    "    # Definir l√≠mites de outliers\n",
    "    outlier_limits = {\n",
    "        'GHI': 1200,  # Actualizado para coincidir con limpiar_TMY_completo\n",
    "        'DNI': 1300,\n",
    "        'DHI': 600\n",
    "    }\n",
    "\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"=== Informe EDA - {location} ===\\n\\n\")\n",
    "\n",
    "        # 1. Informaci√≥n general\n",
    "        f.write(\"1. INFORMACI√ìN GENERAL\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"N√∫mero total de registros: {len(df)}\\n\")\n",
    "        f.write(f\"Per√≠odo: {df['Month'].min()}/{df['Day'].min()} - {df['Month'].max()}/{df['Day'].max()}\\n\\n\")\n",
    "\n",
    "        # 2. An√°lisis de valores faltantes\n",
    "        f.write(\"2. AN√ÅLISIS DE VALORES FALTANTES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        nan_counts = df[['GHI', 'DNI', 'DHI']].isna().sum()\n",
    "        nan_percentages = (nan_counts / len(df)) * 100\n",
    "\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            f.write(f\"  - N√∫mero de valores faltantes: {nan_counts[col]}\\n\")\n",
    "            f.write(f\"  - Porcentaje de valores faltantes: {nan_percentages[col]:.2f}%\\n\")\n",
    "\n",
    "            # An√°lisis de secuencias de NaN\n",
    "            if nan_counts[col] > 0:\n",
    "                nan_sequences = df[col].isna().astype(int).groupby(\n",
    "                    (df[col].isna().astype(int).diff() != 0).cumsum()\n",
    "                ).cumsum()\n",
    "                max_consecutive = nan_sequences.max()\n",
    "                f.write(f\"  - M√°xima secuencia de NaN consecutivos: {max_consecutive}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 3. An√°lisis de Outliers\n",
    "        f.write(\"3. AN√ÅLISIS DE OUTLIERS\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(\"Criterios de outliers:\\n\")\n",
    "        f.write(\"- GHI > 1200 W/m¬≤ o < 0\\n\")  # Actualizado\n",
    "        f.write(\"- DNI > 1300 W/m¬≤ o < 0\\n\")\n",
    "        f.write(\"- DHI > 600 W/m¬≤ o < 0\\n\\n\")\n",
    "\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            # Identificar outliers (valores negativos o mayores al l√≠mite)\n",
    "            neg_outliers = df[df[col] < 0][col]\n",
    "            high_outliers = df[df[col] > outlier_limits[col]][col]\n",
    "            total_outliers = len(neg_outliers) + len(high_outliers)\n",
    "\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            f.write(f\"  - N√∫mero total de outliers: {total_outliers}\\n\")\n",
    "            f.write(f\"  - Porcentaje de outliers: {(total_outliers/len(df))*100:.2f}%\\n\")\n",
    "\n",
    "            if len(neg_outliers) > 0:\n",
    "                f.write(f\"  - Outliers negativos: {len(neg_outliers)} ({len(neg_outliers)/total_outliers*100:.2f}% del total de outliers)\\n\")\n",
    "                f.write(f\"    * Valor m√≠nimo: {neg_outliers.min():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Valor m√°ximo negativo: {neg_outliers.max():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Media de outliers negativos: {neg_outliers.mean():.2f} W/m¬≤\\n\")\n",
    "\n",
    "            if len(high_outliers) > 0:\n",
    "                f.write(f\"  - Outliers altos: {len(high_outliers)} ({len(high_outliers)/total_outliers*100:.2f}% del total de outliers)\\n\")\n",
    "                f.write(f\"    * Valor m√≠nimo: {high_outliers.min():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Valor m√°ximo: {high_outliers.max():.2f} W/m¬≤\\n\")\n",
    "                f.write(f\"    * Media de outliers altos: {high_outliers.mean():.2f} W/m¬≤\\n\")\n",
    "\n",
    "            # An√°lisis temporal de outliers\n",
    "            if total_outliers > 0:\n",
    "                f.write(\"  - Distribuci√≥n temporal de outliers:\\n\")\n",
    "                for month in range(1, 13):\n",
    "                    month_outliers = len(df[(df['Month'] == month) & \n",
    "                                          ((df[col] < 0) | (df[col] > outlier_limits[col]))])\n",
    "                    if month_outliers > 0:\n",
    "                        f.write(f\"    * Mes {month}: {month_outliers} outliers\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # 4. Estad√≠sticas descriptivas (excluyendo outliers)\n",
    "        f.write(\"4. ESTAD√çSTICAS DESCRIPTIVAS (EXCLUYENDO OUTLIERS)\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            # Filtrar valores v√°lidos (no outliers)\n",
    "            valid_data = df[(df[col] >= 0) & (df[col] <= outlier_limits[col])][col]\n",
    "            stats = valid_data.describe()\n",
    "            f.write(f\"  - Media: {stats['mean']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Desviaci√≥n est√°ndar: {stats['std']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - M√≠nimo: {stats['min']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - M√°ximo: {stats['max']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Mediana: {stats['50%']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Q1 (25%): {stats['25%']:.2f} W/m¬≤\\n\")\n",
    "            f.write(f\"  - Q3 (75%): {stats['75%']:.2f} W/m¬≤\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 5. Recomendaciones\n",
    "        f.write(\"5. RECOMENDACIONES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            if nan_counts[col] > 0:\n",
    "                f.write(f\"  - Considerar interpolaci√≥n para {nan_counts[col]} valores faltantes \")\n",
    "                if 'max_consecutive' in locals() and max_consecutive > 4:\n",
    "                    f.write(f\"(¬°Alerta! Hay secuencias de hasta {max_consecutive} NaN consecutivos)\\n\")\n",
    "                else:\n",
    "                    f.write(\"(secuencias cortas, adecuadas para interpolaci√≥n)\\n\")\n",
    "\n",
    "            neg_count = len(df[df[col] < 0])\n",
    "            high_count = len(df[df[col] > outlier_limits[col]])\n",
    "            if neg_count > 0 or high_count > 0:\n",
    "                f.write(f\"  - Reemplazar {neg_count + high_count} outliers:\\n\")\n",
    "                if neg_count > 0:\n",
    "                    f.write(f\"    * {neg_count} valores negativos con 0\\n\")\n",
    "                if high_count > 0:\n",
    "                    f.write(f\"    * {high_count} valores > {outlier_limits[col]} W/m¬≤ con {outlier_limits[col]} W/m¬≤\\n\")\n",
    "                f.write(f\"  - Revisar la calidad de los datos en los meses con mayor concentraci√≥n de outliers\\n\")\n",
    "\n",
    "        f.write(\"\\nRecomendaciones generales:\\n\")\n",
    "        f.write(\"1. Reemplazar todos los valores negativos con 0\\n\")\n",
    "        f.write(\"2. Limitar los valores m√°ximos a los umbrales f√≠sicos:\\n\")\n",
    "        f.write(\"   - GHI: 1200 W/m¬≤\\n\")  # Actualizado\n",
    "        f.write(\"   - DNI: 1300 W/m¬≤\\n\")\n",
    "        f.write(\"   - DHI: 600 W/m¬≤\\n\")\n",
    "        f.write(\"3. Considerar la interpolaci√≥n solo para secuencias cortas de NaN (‚â§ 4 horas)\\n\")\n",
    "        f.write(\"4. Revisar la calidad de los datos en los meses con mayor concentraci√≥n de outliers\\n\")\n",
    "        f.write(\"5. Documentar el proceso de limpieza y las decisiones tomadas para el manejo de outliers\\n\")\n",
    "        # --- SECCI√ìN DE RESULTADOS FINALES ---\n",
    "        if df_final is not None:\n",
    "            print(f\"[DEBUG] Escribiendo resultados finales para {location}...\")\n",
    "            f.write(\"\\n=== RESULTADOS FINALES DE LA LIMPIEZA ===\\n\")\n",
    "            outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 600}\n",
    "            for col in ['GHI', 'DNI', 'DHI']:\n",
    "                neg_outliers = len(df_final[df_final[col] < 0])\n",
    "                high_outliers = len(df_final[df_final[col] > outlier_limits[col]])\n",
    "                total_outliers = neg_outliers + high_outliers\n",
    "                nans = df_final[col].isna().sum()\n",
    "                f.write(f\"{col}: {total_outliers} outliers (Negativos: {neg_outliers}, Altos: {high_outliers}), {nans} NaNs\\n\")\n",
    "\n",
    "def analisis_estacional(df, location):\n",
    "    \"\"\"\n",
    "    Realiza un an√°lisis estacional de los datos solares.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== AN√ÅLISIS ESTACIONAL - {location} ===\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Definir estaciones\n",
    "    estaciones = {\n",
    "        'Verano': [12, 1, 2],\n",
    "        'Oto√±o': [3, 4, 5],\n",
    "        'Invierno': [6, 7, 8],\n",
    "        'Primavera': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    # Crear DataFrame para almacenar estad√≠sticas estacionales\n",
    "    stats_estacionales = pd.DataFrame(index=estaciones.keys(), \n",
    "                                    columns=['GHI Promedio', 'DNI Promedio', 'DHI Promedio',\n",
    "                                            'GHI M√°ximo', 'DNI M√°ximo', 'DHI M√°ximo',\n",
    "                                            'Horas de Sol', 'Energ√≠a Total'])\n",
    "\n",
    "    # Calcular estad√≠sticas por estaci√≥n\n",
    "    for estacion, meses in estaciones.items():\n",
    "        df_estacion = df[df['Month'].isin(meses)]\n",
    "\n",
    "        # Estad√≠sticas b√°sicas\n",
    "        stats_estacionales.loc[estacion, 'GHI Promedio'] = df_estacion['GHI'].mean()\n",
    "        stats_estacionales.loc[estacion, 'DNI Promedio'] = df_estacion['DNI'].mean()\n",
    "        stats_estacionales.loc[estacion, 'DHI Promedio'] = df_estacion['DHI'].mean()\n",
    "\n",
    "        stats_estacionales.loc[estacion, 'GHI M√°ximo'] = df_estacion['GHI'].max()\n",
    "        stats_estacionales.loc[estacion, 'DNI M√°ximo'] = df_estacion['DNI'].max()\n",
    "        stats_estacionales.loc[estacion, 'DHI M√°ximo'] = df_estacion['DHI'].max()\n",
    "\n",
    "        # Horas de sol y energ√≠a\n",
    "        stats_estacionales.loc[estacion, 'Horas de Sol'] = len(df_estacion[df_estacion['GHI'] > 0])\n",
    "        stats_estacionales.loc[estacion, 'Energ√≠a Total'] = df_estacion['GHI'].sum() / 1000\n",
    "\n",
    "    # Mostrar estad√≠sticas estacionales\n",
    "    print(\"\\nEstad√≠sticas por Estaci√≥n:\")\n",
    "    print(stats_estacionales.round(2))\n",
    "\n",
    "\n",
    "def marcar_outliers_nan(df):\n",
    "    \"\"\"\n",
    "    Marca como NaN los valores f√≠sicamente inv√°lidos en GHI, DNI y DHI.\n",
    "    - GHI < 0 o GHI > 1400\n",
    "    - DNI < 0 o DNI > 1300\n",
    "    - DHI < 0 o DHI > 400\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['GHI'] = df['GHI'].mask((df['GHI'] < 0) | (df['GHI'] > 1400))\n",
    "    df['DNI'] = df['DNI'].mask((df['DNI'] < 0) | (df['DNI'] > 1300))\n",
    "    df['DHI'] = df['DHI'].mask((df['DHI'] < 0) | (df['DHI'] > 600))\n",
    "    return df\n",
    "\n",
    "\n",
    "def limpiar_TMY_completo(archivo_entrada, archivo_salida, max_ghi=1400, max_dni=1300, max_dhi=600, interp_limit=6, location=None):\n",
    "    \"\"\"\n",
    "    Limpia GHI, DNI, DHI, Tdry, Tdew, RH y Pres de un archivo TMY artificial y guarda un solo archivo limpio con metadatos.\n",
    "    \"\"\"\n",
    "    # Leer metadatos\n",
    "    with open(archivo_entrada, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(2)]\n",
    "    # Leer datos desde la tercera l√≠nea\n",
    "    df = pd.read_csv(archivo_entrada, skiprows=2)\n",
    "    # Crear columna datetime y usar como √≠ndice\n",
    "    df[\"datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # Ajustar l√≠mites seg√∫n la ubicaci√≥n\n",
    "    if location == \"Vallenar\":\n",
    "        max_ghi = 1200\n",
    "        ghi_high_season = 1150\n",
    "    else:\n",
    "        max_ghi = 1400\n",
    "        ghi_high_season = 1250\n",
    "\n",
    "    # Limpiar GHI con l√≠mites dependientes del mes y ubicaci√≥n\n",
    "    df[\"GHI\"] = df[\"GHI\"].mask(\n",
    "        ((df[\"Month\"].between(3, 10)) & (df[\"GHI\"] >= ghi_high_season)) |  # L√≠mite para meses de alta radiaci√≥n\n",
    "        ((~df[\"Month\"].between(3, 10)) & (df[\"GHI\"] > max_ghi)) |  # L√≠mite general\n",
    "        (df[\"GHI\"] < 0)\n",
    "    )\n",
    "    df[\"GHI\"] = df[\"GHI\"].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Limpiar DNI\n",
    "    df[\"DNI\"] = df[\"DNI\"].mask((df[\"DNI\"] < 0) | (df[\"DNI\"] > max_dni))\n",
    "    df[\"DNI\"] = df[\"DNI\"].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Limpiar DHI con chequeos f√≠sicos avanzados\n",
    "    # Calcular posici√≥n solar para validaci√≥n f√≠sica\n",
    "    lat = -26.2533 if location == \"Salvador\" else -22.4661 if location == \"Calama\" else -28.5766\n",
    "    lon = -69.0522 if location == \"Salvador\" else -68.9244 if location == \"Calama\" else -70.7601\n",
    "    solar_position = get_solarposition(df.index, latitude=lat, longitude=lon)\n",
    "    cos_zenith = np.cos(np.radians(solar_position[\"zenith\"]))\n",
    "    dhi_est = (df[\"GHI\"] - df[\"DNI\"] * cos_zenith).clip(lower=0)\n",
    "\n",
    "    cond_invalid_dhi = (\n",
    "        (df[\"DHI\"] < 0) |\n",
    "        (df[\"DHI\"] > max_dhi) |\n",
    "        (df[\"DHI\"] > df[\"GHI\"]) |\n",
    "        (df[\"DHI\"] > dhi_est + 30) |\n",
    "        (df[\"DHI\"] > 0.95 * df[\"GHI\"]) |\n",
    "        ((solar_position[\"zenith\"] > 90) & (df[\"DHI\"] > 5))\n",
    "    )\n",
    "    df.loc[cond_invalid_dhi, \"DHI\"] = np.nan\n",
    "\n",
    "    # Interpolaci√≥n robusta para DHI\n",
    "    def interpolar_robusto(serie, limit):\n",
    "        nan_groups = serie.isna().astype(int).groupby(serie.notna().astype(int).cumsum()).sum()\n",
    "        if (nan_groups > limit).any():\n",
    "            mask = serie.isna()\n",
    "            for idx, size in nan_groups[nan_groups > limit].items():\n",
    "                mask[mask.groupby(mask.cumsum()).ngroup() == idx] = False\n",
    "            serie_interp = serie.interpolate(method='linear', limit=limit, limit_direction='both')\n",
    "            serie[mask] = serie_interp[mask]\n",
    "            return serie\n",
    "        else:\n",
    "            return serie.interpolate(method='linear', limit=limit, limit_direction='both')\n",
    "\n",
    "    df[\"DHI\"] = interpolar_robusto(df[\"DHI\"], interp_limit)\n",
    "\n",
    "    # Limpiar Tdry, Tdew, RH y Pres\n",
    "    # Definir l√≠mites m√°s realistas para cada par√°metro\n",
    "    temp_limits = {\n",
    "        'Tdry': (-10, 50),  # Limitar entre -10¬∞C y 50¬∞C\n",
    "        'Tdew': (-10, 50),  # Limitar entre -10¬∞C y 50¬∞C\n",
    "        'RH': (1, 100),     # Forzar al rango f√≠sico [1%, 100%]\n",
    "        'Pres': (760, 790)  # Limitar entre 760 y 790 hPa\n",
    "    }\n",
    "\n",
    "    for param, (min_val, max_val) in temp_limits.items():\n",
    "        df[param] = df[param].mask((df[param] < min_val) | (df[param] > max_val))\n",
    "        df[param] = df[param].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Interpolaci√≥n para corregir NaNs\n",
    "    for col in ['GHI', 'DNI', 'DHI', 'Tdry', 'Tdew', 'RH', 'Pres']:\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Opcional: Rellenar cualquier NaN restante con la media de la columna\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Restaurar columnas separadas\n",
    "    df = df.reset_index()\n",
    "    df[\"Year\"] = df[\"datetime\"].dt.year\n",
    "    df[\"Month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"Day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"Hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"Minute\"] = df[\"datetime\"].dt.minute\n",
    "\n",
    "    # Reordenar columnas\n",
    "    columnas_fecha = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]\n",
    "    columnas_finales = columnas_fecha + [col for col in df.columns if col not in columnas_fecha + [\"datetime\"]]\n",
    "    df = df[columnas_finales]\n",
    "\n",
    "    # --- NUEVOS OUTLIERS ESPEC√çFICOS POR UBICACI√ìN Y MES ---\n",
    "    if location == \"Vallenar\":\n",
    "        # GHI > 1000 entre abril y agosto\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(4, 8)) & (df[\"GHI\"] > 1000))\n",
    "        # DNI > 1150 todo el a√±o\n",
    "        df[\"DNI\"] = df[\"DNI\"].mask(df[\"DNI\"] > 1150)\n",
    "    elif location == \"Calama\":\n",
    "        # GHI > 1000 entre mayo y julio\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(5, 7)) & (df[\"GHI\"] > 1000))\n",
    "        # DNI > 1200 todo el a√±o\n",
    "        df[\"DNI\"] = df[\"DNI\"].mask(df[\"DNI\"] > 1200)\n",
    "    elif location == \"Salvador\":\n",
    "        # GHI > 1100 entre abril y agosto\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(4, 8)) & (df[\"GHI\"] > 1100))\n",
    "    # --- FIN NUEVOS OUTLIERS ---\n",
    "\n",
    "    # Despu√©s de limpiar los datos\n",
    "    df_limpio = df  # Aseg√∫rate de que df_limpio est√© definido\n",
    "    revisar_outliers_final(df_limpio, location)\n",
    "\n",
    "    # Guardar nuevo archivo con metadatos originales\n",
    "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "    print(f\"‚úÖ Archivo limpio guardado en: {archivo_salida}\")\n",
    "\n",
    "\n",
    "def revisar_outliers_final(df, location):\n",
    "    \"\"\"\n",
    "    Revisa la existencia de outliers y NaNs en los datos limpios.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - {location} ===\")\n",
    "    outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 600}  # L√≠mites de outliers\n",
    "\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        neg_outliers = len(df[df[col] < 0])\n",
    "        high_outliers = len(df[df[col] > outlier_limits[col]])\n",
    "        total_outliers = neg_outliers + high_outliers\n",
    "        nans = df[col].isna().sum()\n",
    "        print(f\"{col}: {total_outliers} outliers, {nans} NaNs\")\n",
    "        if total_outliers > 0:\n",
    "            print(f\"  - Negativos: {neg_outliers}\")\n",
    "            print(f\"  - Altos: {high_outliers}\")\n",
    "        if nans > 0:\n",
    "            print(f\"  - NaNs: {nans}\")\n",
    "\n",
    "\n",
    "def generar_resumen_comparativo(dfs, locations):\n",
    "    \"\"\"\n",
    "    Genera un resumen comparativo del EDA para todas las ubicaciones.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RESUMEN COMPARATIVO DE DATOS SOLARES ===\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # An√°lisis de valores faltantes\n",
    "    print(\"\\nAn√°lisis de Valores Faltantes:\")\n",
    "    for df, location in zip(dfs, locations):\n",
    "        nan_counts = df[['GHI', 'DNI', 'DHI']].isna().sum()\n",
    "        print(f\"\\n{location}:\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            print(f\"  {col}: {nan_counts[col]} valores faltantes ({(nan_counts[col]/len(df))*100:.2f}%)\")\n",
    "\n",
    "    # An√°lisis de outliers\n",
    "    print(\"\\nAn√°lisis de Outliers:\")\n",
    "    outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 400}  # Actualizado\n",
    "    for df, location in zip(dfs, locations):\n",
    "        print(f\"\\n{location}:\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            neg_outliers = len(df[df[col] < 0])\n",
    "            high_outliers = len(df[df[col] > outlier_limits[col]])\n",
    "            total_outliers = neg_outliers + high_outliers\n",
    "            print(f\"  {col}: {total_outliers} outliers ({(total_outliers/len(df))*100:.2f}%)\")\n",
    "            if total_outliers > 0:\n",
    "                print(f\"    - Negativos: {neg_outliers}\")\n",
    "                print(f\"    - Altos: {high_outliers}\")\n",
    "\n",
    "\n",
    "def reconstruir_TMY_con_fechas_originales(archivo_corrupto, archivo_limpio, archivo_salida):\n",
    "    \"\"\"\n",
    "    Genera un nuevo archivo TMY limpio con los a√±os y meses originales del archivo fuente,\n",
    "    pero con los valores limpios. Mantiene los metadatos y el encabezado.\n",
    "    \"\"\"\n",
    "    # Leer datos limpios\n",
    "    df_limpio = pd.read_csv(archivo_limpio, skiprows=2)\n",
    "    # Leer datos originales (sin metadatos)\n",
    "    df_original = pd.read_csv(archivo_corrupto)\n",
    "    # Reemplazar columnas de fecha por las originales\n",
    "    for col in ['Year', 'Month', 'Day', 'Hour', 'Minute']:\n",
    "        if col in df_original.columns and col in df_limpio.columns:\n",
    "            df_limpio[col] = df_original[col].values\n",
    "    # Reordenar columnas para mantener el formato\n",
    "    columnas_fecha = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    columnas_finales = columnas_fecha + [col for col in df_limpio.columns if col not in columnas_fecha]\n",
    "    df_limpio = df_limpio[columnas_finales]\n",
    "    # Leer metadatos (primeras 3 l√≠neas del archivo limpio)\n",
    "    with open(archivo_limpio, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(3)]\n",
    "    # Guardar el nuevo archivo\n",
    "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        df_limpio.to_csv(f, index=False, header=False)\n",
    "    print(f\"‚úÖ Archivo TMY limpio con fechas originales guardado en: {archivo_salida}\")\n",
    "\n",
    "\n",
    "def scan_nan_gaps(df):\n",
    "    \"\"\"\n",
    "    Muestra cu√°ntos NaN y los huecos m√°s largos en GHI, DNI, DHI.\n",
    "    \"\"\"\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            nan_sequences = df[col].isna().astype(int).groupby(\n",
    "                (df[col].isna().astype(int).diff() != 0).cumsum()\n",
    "            ).cumsum()\n",
    "            max_consecutive = nan_sequences.max()\n",
    "            print(f\"{col}: {nan_count} NaNs, Max gap: {max_consecutive} hours\")\n",
    "        else:\n",
    "            print(f\"{col}: No NaNs\")\n",
    "\n",
    "\n",
    "def fill_nan_hierarchical(df):\n",
    "    \"\"\"\n",
    "    1. Interpola huecos ‚â§ 3 h\n",
    "    2. Rellena lo restante con el promedio Mes-Hora\n",
    "    3. Si a√∫n falta, con el promedio anual por Hora\n",
    "    \"\"\"\n",
    "    # Interpolaci√≥n para huecos peque√±os\n",
    "    df.interpolate(method='linear', limit=3, limit_direction='both', inplace=True)\n",
    "\n",
    "    # Rellenar con percentil 75 Mes-Hora\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df.groupby(['Month', 'Hour'])[col].transform(lambda x: x.fillna(x.quantile(0.75)))\n",
    "\n",
    "    # Rellenar con percentil 75 anual por Hora\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df.groupby('Hour')[col].transform(lambda x: x.fillna(x.quantile(0.75)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_and_save_final_tmy(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Lee el archivo CSV, aplica las funciones de manejo de NaN y guarda el resultado.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(2)]  # Leer las dos primeras l√≠neas de metadatos\n",
    "    df = pd.read_csv(file_path, skiprows=2)\n",
    "\n",
    "    # Poner la columna datetime como √≠ndice\n",
    "    df['datetime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # Escanear huecos de NaN\n",
    "    print(f\"\\nEscaneando huecos de NaN en {file_path} antes de llenar:\")\n",
    "    scan_nan_gaps(df)\n",
    "\n",
    "    # Llenar NaN jer√°rquicamente\n",
    "    df = fill_nan_hierarchical(df)\n",
    "\n",
    "    # Escanear nuevamente para confirmar que no quedan NaN\n",
    "    print(f\"\\nEscaneando huecos de NaN en {file_path} despu√©s de llenar:\")\n",
    "    scan_nan_gaps(df)\n",
    "\n",
    "    # Guardar el resultado con el mismo formato TMY\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        f.write(','.join(df.columns) + '\\n')  # Escribir encabezado de columnas\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "    print(f\"‚úÖ Archivo final guardado en: {output_path}\")\n",
    "\n",
    "\n",
    "def main_tmy():\n",
    "    print(\"Iniciando procesamiento...\")\n",
    "    # Definir los metadatos para cada sitio\n",
    "    metadatos_salvador = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00001\", \"City\": \"Salvador\", \"State\": \"Atacama\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -26.2533, \"Longitude\": -69.0522, \"Time Zone\": -4, \"Elevation\": 2280\n",
    "    }\n",
    "    metadatos_calama = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00002\", \"City\": \"Calama\", \"State\": \"Antofagasta\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -22.4661, \"Longitude\": -68.9244, \"Time Zone\": -4, \"Elevation\": 2260\n",
    "    }\n",
    "    metadatos_vallenar = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00003\", \"City\": \"Vallenar\", \"State\": \"Atacama\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -28.5766, \"Longitude\": -70.7601, \"Time Zone\": -4, \"Elevation\": 441\n",
    "    }\n",
    "    # Rutas de archivos\n",
    "    path_salvador = Path(\"salvador_corrupted.csv\")\n",
    "    path_calama = Path(\"calama_corrupted.csv\")\n",
    "    path_vallenar = Path(\"Vallenar_corrupted.csv\")\n",
    "    # Salidas\n",
    "    output_salvador = Path(\"salvador_TMY_artificial.csv\")\n",
    "    output_calama = Path(\"calama_TMY_artificial.csv\")\n",
    "    output_vallenar = Path(\"vallenar_TMY_artificial.csv\")\n",
    "    output_salvador_limpio = Path(DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\")\n",
    "    output_calama_limpio = Path(DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\")\n",
    "    output_vallenar_limpio = Path(DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\")\n",
    "\n",
    "    # Procesar cada archivo (sin generar el informe EDA aqu√≠)\n",
    "    for location, (input_path, output_path, output_limpio, metadata) in [\n",
    "        (\"Salvador\", (path_salvador, output_salvador, output_salvador_limpio, metadatos_salvador)),\n",
    "        (\"Calama\", (path_calama, output_calama, output_calama_limpio, metadatos_calama)),\n",
    "        (\"Vallenar\", (path_vallenar, output_vallenar, output_vallenar_limpio, metadatos_vallenar))\n",
    "    ]:\n",
    "        try:\n",
    "            print(f\"\\nProcesando {location}...\")\n",
    "            print(f\"Leyendo archivo: {input_path}\")\n",
    "            df = transformar_a_tmy_con_metadatos(input_path, output_path, metadata)\n",
    "            print(f\"Transformaci√≥n completada. Limpiando datos...\")\n",
    "            limpiar_TMY_completo(str(output_path), str(output_limpio), location=location)\n",
    "            print(f\"Limpieza completada. Eliminando archivos temporales...\")\n",
    "            for suffix in [\"_GHI_limpio.csv\", \"_DNI_limpio.csv\", \"_DHI_limpio.csv\"]:\n",
    "                try:\n",
    "                    os.remove(str(output_path).replace(\"_TMY_artificial.csv\", suffix))\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            print(f\"Leyendo archivo limpio...\")\n",
    "            df_limpio = pd.read_csv(output_limpio, skiprows=2)\n",
    "            print(f\"Generando gr√°ficos...\")\n",
    "            plot_tmy_data(df_limpio, location)\n",
    "            print(f\"Realizando an√°lisis estacional...\")\n",
    "            analisis_estacional(df_limpio, location)\n",
    "            print(f\"Eliminando archivo temporal...\")\n",
    "            try:\n",
    "                os.remove(str(output_path))\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            print(f\"Procesamiento de {location} completado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {location}: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    # Generar resumen comparativo\n",
    "    dfs_limpios = [\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\", skiprows=2),\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\", skiprows=2),\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\", skiprows=2)\n",
    "    ]\n",
    "    locations = [\"Salvador\", \"Calama\", \"Vallenar\"]\n",
    "    if len(dfs_limpios) == 3:\n",
    "        print(\"\\nGenerando resumen comparativo...\")\n",
    "        generar_resumen_comparativo(dfs_limpios, locations)\n",
    "        print(\"\\nReconstruyendo archivos TMY limpios con fechas originales...\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"salvador_corrupted.csv\", DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio_originales.csv\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"calama_corrupted.csv\", DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"calama_TMY_limpio_originales.csv\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"Vallenar_corrupted.csv\", DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio_originales.csv\")\n",
    "    else:\n",
    "        print(f\"\\nNo se pudo generar el resumen comparativo. Se procesaron {len(dfs_limpios)} de 3 archivos.\")\n",
    "\n",
    "    # Procesar cada archivo limpio y guardar el resultado final\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'calama_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'calama_TMY_final.csv')\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'salvador_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'salvador_TMY_final.csv')\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'vallenar_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'vallenar_TMY_final.csv')\n",
    "\n",
    "    # Ahora s√≠, generar el informe EDA con ambos DataFrames\n",
    "    for location in [\"Calama\", \"Salvador\", \"Vallenar\"]:\n",
    "        df_limpio = pd.read_csv(DATOS_LIMPIOS_DIR / f\"{location.lower()}_TMY_limpio.csv\", skiprows=2)\n",
    "        df_final = pd.read_csv(DATOS_LIMPIOS_DIR / f\"{location.lower()}_TMY_final.csv\", skiprows=2)\n",
    "        generate_eda_report(df_limpio, location, df_final=df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21518998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento...\n",
      "\n",
      "Procesando Salvador...\n",
      "Leyendo archivo: salvador_corrupted.csv\n",
      "Transformaci√≥n completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/2193541072.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - Salvador ===\n",
      "GHI: 111 outliers, 1 NaNs\n",
      "  - Negativos: 0\n",
      "  - Altos: 111\n",
      "  - NaNs: 1\n",
      "DNI: 0 outliers, 0 NaNs\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "‚úÖ Archivo limpio guardado en: datos_limpios/salvador_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gr√°ficos...\n",
      "Realizando an√°lisis estacional...\n",
      "\n",
      "=== AN√ÅLISIS ESTACIONAL - Salvador ===\n",
      "==================================================\n",
      "\n",
      "Estad√≠sticas por Estaci√≥n:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI M√°ximo DNI M√°ximo  \\\n",
      "Verano      389.684931   541.464676    15.965579     1248.3     1222.7   \n",
      "Oto√±o       258.563072   406.613972    14.172056     1167.7     1203.6   \n",
      "Invierno    197.343727   337.040602    11.540444      978.0     1149.7   \n",
      "Primavera   354.186401   515.705128    13.635943     1246.4     1223.6   \n",
      "\n",
      "          DHI M√°ximo Horas de Sol Energ√≠a Total  \n",
      "Verano         518.2         1201     841.71945  \n",
      "Oto√±o          535.7         1039      570.6487  \n",
      "Invierno       164.5          970     435.73495  \n",
      "Primavera      177.5         1143      773.5431  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Salvador completado.\n",
      "\n",
      "Procesando Calama...\n",
      "Leyendo archivo: calama_corrupted.csv\n",
      "Transformaci√≥n completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/2193541072.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - Calama ===\n",
      "GHI: 49 outliers, 3 NaNs\n",
      "  - Negativos: 0\n",
      "  - Altos: 49\n",
      "  - NaNs: 3\n",
      "DNI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "‚úÖ Archivo limpio guardado en: datos_limpios/calama_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gr√°ficos...\n",
      "Realizando an√°lisis estacional...\n",
      "\n",
      "=== AN√ÅLISIS ESTACIONAL - Calama ===\n",
      "==================================================\n",
      "\n",
      "Estad√≠sticas por Estaci√≥n:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI M√°ximo DNI M√°ximo  \\\n",
      "Verano       371.25412   484.361458    16.991319     1254.0     1192.9   \n",
      "Oto√±o        276.07951    425.86216    14.890082     1156.1     1163.1   \n",
      "Invierno    223.032465     379.3744    11.958243      985.3     1127.4   \n",
      "Primavera   351.340911   488.193816    17.601717     1233.8     1196.4   \n",
      "\n",
      "          DHI M√°ximo Horas de Sol Energ√≠a Total  \n",
      "Verano         432.1         1196      801.9089  \n",
      "Oto√±o          349.8         1060      609.0314  \n",
      "Invierno       192.0          987     492.23265  \n",
      "Primavera      527.6         1135     767.32855  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Calama completado.\n",
      "\n",
      "Procesando Vallenar...\n",
      "Leyendo archivo: Vallenar_corrupted.csv\n",
      "Transformaci√≥n completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/2193541072.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISI√ìN FINAL DE OUTLIERS Y NaNs - Vallenar ===\n",
      "GHI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DNI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "‚úÖ Archivo limpio guardado en: datos_limpios/vallenar_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gr√°ficos...\n",
      "Realizando an√°lisis estacional...\n",
      "\n",
      "=== AN√ÅLISIS ESTACIONAL - Vallenar ===\n",
      "==================================================\n",
      "\n",
      "Estad√≠sticas por Estaci√≥n:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI M√°ximo DNI M√°ximo  \\\n",
      "Verano      352.003056   464.676795    21.866458     1152.6     1111.0   \n",
      "Oto√±o        217.90401   326.889253    16.282994     1049.1     1074.7   \n",
      "Invierno    156.989964   266.652447    14.600317      864.3     1036.3   \n",
      "Primavera   302.854806   393.443402    18.393246     1164.0     1132.5   \n",
      "\n",
      "          DHI M√°ximo Horas de Sol Energ√≠a Total  \n",
      "Verano         268.5         1181      760.3266  \n",
      "Oto√±o          260.5         1029     480.91415  \n",
      "Invierno       332.6          961     346.47685  \n",
      "Primavera      235.0         1149    661.434897  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Vallenar completado.\n",
      "\n",
      "Generando resumen comparativo...\n",
      "\n",
      "=== RESUMEN COMPARATIVO DE DATOS SOLARES ===\n",
      "==================================================\n",
      "\n",
      "An√°lisis de Valores Faltantes:\n",
      "\n",
      "Salvador:\n",
      "  GHI: 1 valores faltantes (0.01%)\n",
      "  DNI: 0 valores faltantes (0.00%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Calama:\n",
      "  GHI: 3 valores faltantes (0.03%)\n",
      "  DNI: 2 valores faltantes (0.02%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Vallenar:\n",
      "  GHI: 2 valores faltantes (0.02%)\n",
      "  DNI: 2 valores faltantes (0.02%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "An√°lisis de Outliers:\n",
      "\n",
      "Salvador:\n",
      "  GHI: 111 outliers (1.27%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 111\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 5 outliers (0.06%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 5\n",
      "\n",
      "Calama:\n",
      "  GHI: 49 outliers (0.56%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 49\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 4 outliers (0.05%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 4\n",
      "\n",
      "Vallenar:\n",
      "  GHI: 0 outliers (0.00%)\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 0 outliers (0.00%)\n",
      "\n",
      "Reconstruyendo archivos TMY limpios con fechas originales...\n",
      "‚úÖ Archivo TMY limpio con fechas originales guardado en: datos_limpios/salvador_TMY_limpio_originales.csv\n",
      "‚úÖ Archivo TMY limpio con fechas originales guardado en: datos_limpios/calama_TMY_limpio_originales.csv\n",
      "‚úÖ Archivo TMY limpio con fechas originales guardado en: datos_limpios/vallenar_TMY_limpio_originales.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/calama_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 3 NaNs, Max gap: 1 hours\n",
      "DNI: 2 NaNs, Max gap: 1 hours\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/calama_TMY_limpio_originales.csv despu√©s de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "‚úÖ Archivo final guardado en: datos_limpios/calama_TMY_final.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/salvador_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 1 NaNs, Max gap: 1 hours\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/salvador_TMY_limpio_originales.csv despu√©s de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "‚úÖ Archivo final guardado en: datos_limpios/salvador_TMY_final.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/vallenar_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 2 NaNs, Max gap: 1 hours\n",
      "DNI: 2 NaNs, Max gap: 1 hours\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/vallenar_TMY_limpio_originales.csv despu√©s de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "‚úÖ Archivo final guardado en: datos_limpios/vallenar_TMY_final.csv\n",
      "[DEBUG] Generando reporte EDA en: reports/calama_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Calama...\n",
      "[DEBUG] Generando reporte EDA en: reports/salvador_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Salvador...\n",
      "[DEBUG] Generando reporte EDA en: reports/vallenar_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Vallenar...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_tmy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5888fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
