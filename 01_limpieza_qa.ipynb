{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73376cfb",
   "metadata": {},
   "source": [
    "# 🌞 Análisis Exploratorio y Limpieza de Archivos TMY\n",
    "Este notebook transforma, limpia y analiza archivos de datos horarios solares (GHI, DNI y DHI) para Calama, Salvador y Vallenar. Se realiza:\n",
    "\n",
    "- Transformación de datos horarios a formato TMY artificial\n",
    "- Limpieza avanzada de GHI, DNI y DHI incluyendo validación física\n",
    "- Generación de reportes EDA y gráficos\n",
    "- Reconstrucción de archivos limpios con fechas originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a855a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Carga de librerías y definición de funciones\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pvlib\n",
    "import os\n",
    "from pvlib.solarposition import get_solarposition\n",
    "import PySAM.Pvwattsv7 as pv\n",
    "import PySAM.Lcoefcr as Lcoefcr\n",
    "\n",
    "DATOS_LIMPIOS_DIR = Path('datos_limpios')\n",
    "DATOS_LIMPIOS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98325773",
   "metadata": {},
   "source": [
    "## ▶️ Ejecutar la limpieza y calidad de datos para las tres localidades\n",
    "Ejecuta la función `main_tmy()` para procesar los tres archivos TMY, realizar EDA, Limpieza, verificación gráfica de datos, armado de TMY, guardar resultados y generar gráficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f92f1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "def transformar_a_tmy_con_metadatos(csv_path, output_path, metadata_dict):\n",
    "    \"\"\"\n",
    "    Transforma un archivo CSV con datos horarios en un TMY artificial con metadatos en el encabezado.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Eliminar columna 'datetime' si existe\n",
    "    if 'datetime' in df.columns:\n",
    "        df = df.drop(columns=['datetime'])\n",
    "\n",
    "    # Ordenar cronológicamente y recortar o completar a 8760 filas\n",
    "    df = df.sort_values(by=[\"Month\", \"Day\", \"Hour\", \"Minute\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.iloc[:8760]  # en caso de que tenga más filas\n",
    "\n",
    "    # Asignar un año artificial constante\n",
    "    df['Year'] = 1990\n",
    "\n",
    "    # Reordenar columnas: primero las de fecha\n",
    "    columnas_fecha = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    otras = [c for c in df.columns if c not in columnas_fecha]\n",
    "    df = df[columnas_fecha + otras]\n",
    "\n",
    "    # Escribir archivo con las tres primeras líneas de metadatos\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Línea 1: encabezados de metadatos\n",
    "        f.write(\"Source,Location ID,City,State,Country,Latitude,Longitude,Time Zone,Elevation\\n\")\n",
    "        # Línea 2: valores de metadatos\n",
    "        f.write(\",\".join(str(metadata_dict[k]) for k in [\n",
    "            \"Source\", \"Location ID\", \"City\", \"State\", \"Country\",\n",
    "            \"Latitude\", \"Longitude\", \"Time Zone\", \"Elevation\"\n",
    "        ]) + \"\\n\")\n",
    "        # Línea 3: encabezado de columnas de datos\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        # Resto de datos\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_tmy_data(df, location, output_dir=\"plots\", is_clean=False):\n",
    "    \"\"\"\n",
    "    Genera gráficos de los datos TMY artificiales.\n",
    "    \"\"\"\n",
    "    # Crear directorio para gráficos si no existe\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Configurar estilo de gráficos\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # Crear figura con tres subplots\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 15))\n",
    "    title_suffix = \" (Datos Limpios)\" if is_clean else \"\"\n",
    "    fig.suptitle(f'Radiación Solar por Hora - {location}{title_suffix}', fontsize=16, y=0.95)\n",
    "\n",
    "    # Crear fechas para el eje x\n",
    "    dates = [datetime(1990, int(row['Month']), int(row['Day']), int(row['Hour'])) \n",
    "             for _, row in df.iterrows()]\n",
    "\n",
    "    # Graficar cada componente en su propio subplot\n",
    "    ax1.plot(dates, df['GHI'], 'b-', linewidth=1, alpha=0.7)\n",
    "    ax1.set_title('Radiación Global Horizontal (GHI)', fontsize=12)\n",
    "    ax1.set_ylabel('GHI (W/m²)', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1300)  # Establecer límite superior para GHI\n",
    "\n",
    "    ax2.plot(dates, df['DNI'], 'r-', linewidth=1, alpha=0.7)\n",
    "    ax2.set_title('Radiación Normal Directa (DNI)', fontsize=12)\n",
    "    ax2.set_ylabel('DNI (W/m²)', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1300)  # Establecer límite superior para DNI\n",
    "\n",
    "    ax3.plot(dates, df['DHI'], 'g-', linewidth=1, alpha=0.7)\n",
    "    ax3.set_title('Radiación Horizontal Difusa (DHI)', fontsize=12)\n",
    "    ax3.set_ylabel('DHI (W/m²)', fontsize=10)\n",
    "    ax3.set_xlabel('Mes', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 600)  # Establecer límite superior para DHI\n",
    "\n",
    "    # Configurar formato del eje x para todos los subplots\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "    # Ajustar layout y guardar\n",
    "    plt.tight_layout()\n",
    "    suffix = \"_clean\" if is_clean else \"\"\n",
    "    plt.savefig(f'{output_dir}/{location.lower()}_tmy_plots{suffix}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def generate_eda_report(df, location, output_dir=\"reports\", df_final=None):\n",
    "    \"\"\"\n",
    "    Genera un informe EDA con análisis de valores faltantes y outliers.\n",
    "    \"\"\"\n",
    "    # Crear directorio para informes si no existe\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    # Crear archivo de informe\n",
    "    report_path = Path(output_dir) / f\"{location.lower()}_eda_report.txt\"\n",
    "    print(f\"[DEBUG] Generando reporte EDA en: {report_path}\")\n",
    "\n",
    "    # Definir límites de outliers\n",
    "    outlier_limits = {\n",
    "        'GHI': 1200,  # Actualizado para coincidir con limpiar_TMY_completo\n",
    "        'DNI': 1300,\n",
    "        'DHI': 600\n",
    "    }\n",
    "\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"=== Informe EDA - {location} ===\\n\\n\")\n",
    "\n",
    "        # 1. Información general\n",
    "        f.write(\"1. INFORMACIÓN GENERAL\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(f\"Número total de registros: {len(df)}\\n\")\n",
    "        f.write(f\"Período: {df['Month'].min()}/{df['Day'].min()} - {df['Month'].max()}/{df['Day'].max()}\\n\\n\")\n",
    "\n",
    "        # 2. Análisis de valores faltantes\n",
    "        f.write(\"2. ANÁLISIS DE VALORES FALTANTES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        nan_counts = df[['GHI', 'DNI', 'DHI']].isna().sum()\n",
    "        nan_percentages = (nan_counts / len(df)) * 100\n",
    "\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            f.write(f\"  - Número de valores faltantes: {nan_counts[col]}\\n\")\n",
    "            f.write(f\"  - Porcentaje de valores faltantes: {nan_percentages[col]:.2f}%\\n\")\n",
    "\n",
    "            # Análisis de secuencias de NaN\n",
    "            if nan_counts[col] > 0:\n",
    "                nan_sequences = df[col].isna().astype(int).groupby(\n",
    "                    (df[col].isna().astype(int).diff() != 0).cumsum()\n",
    "                ).cumsum()\n",
    "                max_consecutive = nan_sequences.max()\n",
    "                f.write(f\"  - Máxima secuencia de NaN consecutivos: {max_consecutive}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 3. Análisis de Outliers\n",
    "        f.write(\"3. ANÁLISIS DE OUTLIERS\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        f.write(\"Criterios de outliers:\\n\")\n",
    "        f.write(\"- GHI > 1200 W/m² o < 0\\n\")  # Actualizado\n",
    "        f.write(\"- DNI > 1300 W/m² o < 0\\n\")\n",
    "        f.write(\"- DHI > 600 W/m² o < 0\\n\\n\")\n",
    "\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            # Identificar outliers (valores negativos o mayores al límite)\n",
    "            neg_outliers = df[df[col] < 0][col]\n",
    "            high_outliers = df[df[col] > outlier_limits[col]][col]\n",
    "            total_outliers = len(neg_outliers) + len(high_outliers)\n",
    "\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            f.write(f\"  - Número total de outliers: {total_outliers}\\n\")\n",
    "            f.write(f\"  - Porcentaje de outliers: {(total_outliers/len(df))*100:.2f}%\\n\")\n",
    "\n",
    "            if len(neg_outliers) > 0:\n",
    "                f.write(f\"  - Outliers negativos: {len(neg_outliers)} ({len(neg_outliers)/total_outliers*100:.2f}% del total de outliers)\\n\")\n",
    "                f.write(f\"    * Valor mínimo: {neg_outliers.min():.2f} W/m²\\n\")\n",
    "                f.write(f\"    * Valor máximo negativo: {neg_outliers.max():.2f} W/m²\\n\")\n",
    "                f.write(f\"    * Media de outliers negativos: {neg_outliers.mean():.2f} W/m²\\n\")\n",
    "\n",
    "            if len(high_outliers) > 0:\n",
    "                f.write(f\"  - Outliers altos: {len(high_outliers)} ({len(high_outliers)/total_outliers*100:.2f}% del total de outliers)\\n\")\n",
    "                f.write(f\"    * Valor mínimo: {high_outliers.min():.2f} W/m²\\n\")\n",
    "                f.write(f\"    * Valor máximo: {high_outliers.max():.2f} W/m²\\n\")\n",
    "                f.write(f\"    * Media de outliers altos: {high_outliers.mean():.2f} W/m²\\n\")\n",
    "\n",
    "            # Análisis temporal de outliers\n",
    "            if total_outliers > 0:\n",
    "                f.write(\"  - Distribución temporal de outliers:\\n\")\n",
    "                for month in range(1, 13):\n",
    "                    month_outliers = len(df[(df['Month'] == month) & \n",
    "                                          ((df[col] < 0) | (df[col] > outlier_limits[col]))])\n",
    "                    if month_outliers > 0:\n",
    "                        f.write(f\"    * Mes {month}: {month_outliers} outliers\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # 4. Estadísticas descriptivas (excluyendo outliers)\n",
    "        f.write(\"4. ESTADÍSTICAS DESCRIPTIVAS (EXCLUYENDO OUTLIERS)\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            # Filtrar valores válidos (no outliers)\n",
    "            valid_data = df[(df[col] >= 0) & (df[col] <= outlier_limits[col])][col]\n",
    "            stats = valid_data.describe()\n",
    "            f.write(f\"  - Media: {stats['mean']:.2f} W/m²\\n\")\n",
    "            f.write(f\"  - Desviación estándar: {stats['std']:.2f} W/m²\\n\")\n",
    "            f.write(f\"  - Mínimo: {stats['min']:.2f} W/m²\\n\")\n",
    "            f.write(f\"  - Máximo: {stats['max']:.2f} W/m²\\n\")\n",
    "            f.write(f\"  - Mediana: {stats['50%']:.2f} W/m²\\n\")\n",
    "            f.write(f\"  - Q1 (25%): {stats['25%']:.2f} W/m²\\n\")\n",
    "            f.write(f\"  - Q3 (75%): {stats['75%']:.2f} W/m²\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # 5. Recomendaciones\n",
    "        f.write(\"5. RECOMENDACIONES\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            f.write(f\"{col}:\\n\")\n",
    "            if nan_counts[col] > 0:\n",
    "                f.write(f\"  - Considerar interpolación para {nan_counts[col]} valores faltantes \")\n",
    "                if 'max_consecutive' in locals() and max_consecutive > 4:\n",
    "                    f.write(f\"(¡Alerta! Hay secuencias de hasta {max_consecutive} NaN consecutivos)\\n\")\n",
    "                else:\n",
    "                    f.write(\"(secuencias cortas, adecuadas para interpolación)\\n\")\n",
    "\n",
    "            neg_count = len(df[df[col] < 0])\n",
    "            high_count = len(df[df[col] > outlier_limits[col]])\n",
    "            if neg_count > 0 or high_count > 0:\n",
    "                f.write(f\"  - Reemplazar {neg_count + high_count} outliers:\\n\")\n",
    "                if neg_count > 0:\n",
    "                    f.write(f\"    * {neg_count} valores negativos con 0\\n\")\n",
    "                if high_count > 0:\n",
    "                    f.write(f\"    * {high_count} valores > {outlier_limits[col]} W/m² con {outlier_limits[col]} W/m²\\n\")\n",
    "                f.write(f\"  - Revisar la calidad de los datos en los meses con mayor concentración de outliers\\n\")\n",
    "\n",
    "        f.write(\"\\nRecomendaciones generales:\\n\")\n",
    "        f.write(\"1. Reemplazar todos los valores negativos con 0\\n\")\n",
    "        f.write(\"2. Limitar los valores máximos a los umbrales físicos:\\n\")\n",
    "        f.write(\"   - GHI: 1200 W/m²\\n\")  # Actualizado\n",
    "        f.write(\"   - DNI: 1300 W/m²\\n\")\n",
    "        f.write(\"   - DHI: 600 W/m²\\n\")\n",
    "        f.write(\"3. Considerar la interpolación solo para secuencias cortas de NaN (≤ 4 horas)\\n\")\n",
    "        f.write(\"4. Revisar la calidad de los datos en los meses con mayor concentración de outliers\\n\")\n",
    "        f.write(\"5. Documentar el proceso de limpieza y las decisiones tomadas para el manejo de outliers\\n\")\n",
    "        # --- SECCIÓN DE RESULTADOS FINALES ---\n",
    "        if df_final is not None:\n",
    "            print(f\"[DEBUG] Escribiendo resultados finales para {location}...\")\n",
    "            f.write(\"\\n=== RESULTADOS FINALES DE LA LIMPIEZA ===\\n\")\n",
    "            outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 600}\n",
    "            for col in ['GHI', 'DNI', 'DHI']:\n",
    "                neg_outliers = len(df_final[df_final[col] < 0])\n",
    "                high_outliers = len(df_final[df_final[col] > outlier_limits[col]])\n",
    "                total_outliers = neg_outliers + high_outliers\n",
    "                nans = df_final[col].isna().sum()\n",
    "                f.write(f\"{col}: {total_outliers} outliers (Negativos: {neg_outliers}, Altos: {high_outliers}), {nans} NaNs\\n\")\n",
    "\n",
    "def analisis_estacional(df, location):\n",
    "    \"\"\"\n",
    "    Realiza un análisis estacional de los datos solares.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== ANÁLISIS ESTACIONAL - {location} ===\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Definir estaciones\n",
    "    estaciones = {\n",
    "        'Verano': [12, 1, 2],\n",
    "        'Otoño': [3, 4, 5],\n",
    "        'Invierno': [6, 7, 8],\n",
    "        'Primavera': [9, 10, 11]\n",
    "    }\n",
    "\n",
    "    # Crear DataFrame para almacenar estadísticas estacionales\n",
    "    stats_estacionales = pd.DataFrame(index=estaciones.keys(), \n",
    "                                    columns=['GHI Promedio', 'DNI Promedio', 'DHI Promedio',\n",
    "                                            'GHI Máximo', 'DNI Máximo', 'DHI Máximo',\n",
    "                                            'Horas de Sol', 'Energía Total'])\n",
    "\n",
    "    # Calcular estadísticas por estación\n",
    "    for estacion, meses in estaciones.items():\n",
    "        df_estacion = df[df['Month'].isin(meses)]\n",
    "\n",
    "        # Estadísticas básicas\n",
    "        stats_estacionales.loc[estacion, 'GHI Promedio'] = df_estacion['GHI'].mean()\n",
    "        stats_estacionales.loc[estacion, 'DNI Promedio'] = df_estacion['DNI'].mean()\n",
    "        stats_estacionales.loc[estacion, 'DHI Promedio'] = df_estacion['DHI'].mean()\n",
    "\n",
    "        stats_estacionales.loc[estacion, 'GHI Máximo'] = df_estacion['GHI'].max()\n",
    "        stats_estacionales.loc[estacion, 'DNI Máximo'] = df_estacion['DNI'].max()\n",
    "        stats_estacionales.loc[estacion, 'DHI Máximo'] = df_estacion['DHI'].max()\n",
    "\n",
    "        # Horas de sol y energía\n",
    "        stats_estacionales.loc[estacion, 'Horas de Sol'] = len(df_estacion[df_estacion['GHI'] > 0])\n",
    "        stats_estacionales.loc[estacion, 'Energía Total'] = df_estacion['GHI'].sum() / 1000\n",
    "\n",
    "    # Mostrar estadísticas estacionales\n",
    "    print(\"\\nEstadísticas por Estación:\")\n",
    "    print(stats_estacionales.round(2))\n",
    "\n",
    "\n",
    "def marcar_outliers_nan(df):\n",
    "    \"\"\"\n",
    "    Marca como NaN los valores físicamente inválidos en GHI, DNI y DHI.\n",
    "    - GHI < 0 o GHI > 1400\n",
    "    - DNI < 0 o DNI > 1300\n",
    "    - DHI < 0 o DHI > 400\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['GHI'] = df['GHI'].mask((df['GHI'] < 0) | (df['GHI'] > 1400))\n",
    "    df['DNI'] = df['DNI'].mask((df['DNI'] < 0) | (df['DNI'] > 1300))\n",
    "    df['DHI'] = df['DHI'].mask((df['DHI'] < 0) | (df['DHI'] > 600))\n",
    "    return df\n",
    "\n",
    "\n",
    "def limpiar_TMY_completo(archivo_entrada, archivo_salida, max_ghi=1400, max_dni=1300, max_dhi=600, interp_limit=6, location=None):\n",
    "    \"\"\"\n",
    "    Limpia GHI, DNI, DHI, Tdry, Tdew, RH y Pres de un archivo TMY artificial y guarda un solo archivo limpio con metadatos.\n",
    "    \"\"\"\n",
    "    # Leer metadatos\n",
    "    with open(archivo_entrada, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(2)]\n",
    "    # Leer datos desde la tercera línea\n",
    "    df = pd.read_csv(archivo_entrada, skiprows=2)\n",
    "    # Crear columna datetime y usar como índice\n",
    "    df[\"datetime\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]])\n",
    "    df = df.set_index(\"datetime\")\n",
    "\n",
    "    # Ajustar límites según la ubicación\n",
    "    if location == \"Vallenar\":\n",
    "        max_ghi = 1200\n",
    "        ghi_high_season = 1150\n",
    "    else:\n",
    "        max_ghi = 1400\n",
    "        ghi_high_season = 1250\n",
    "\n",
    "    # Limpiar GHI con límites dependientes del mes y ubicación\n",
    "    df[\"GHI\"] = df[\"GHI\"].mask(\n",
    "        ((df[\"Month\"].between(3, 10)) & (df[\"GHI\"] >= ghi_high_season)) |  # Límite para meses de alta radiación\n",
    "        ((~df[\"Month\"].between(3, 10)) & (df[\"GHI\"] > max_ghi)) |  # Límite general\n",
    "        (df[\"GHI\"] < 0)\n",
    "    )\n",
    "    df[\"GHI\"] = df[\"GHI\"].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Limpiar DNI\n",
    "    df[\"DNI\"] = df[\"DNI\"].mask((df[\"DNI\"] < 0) | (df[\"DNI\"] > max_dni))\n",
    "    df[\"DNI\"] = df[\"DNI\"].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Limpiar DHI con chequeos físicos avanzados\n",
    "    # Calcular posición solar para validación física\n",
    "    lat = -26.2533 if location == \"Salvador\" else -22.4661 if location == \"Calama\" else -28.5766\n",
    "    lon = -69.0522 if location == \"Salvador\" else -68.9244 if location == \"Calama\" else -70.7601\n",
    "    solar_position = get_solarposition(df.index, latitude=lat, longitude=lon)\n",
    "    cos_zenith = np.cos(np.radians(solar_position[\"zenith\"]))\n",
    "    dhi_est = (df[\"GHI\"] - df[\"DNI\"] * cos_zenith).clip(lower=0)\n",
    "\n",
    "    cond_invalid_dhi = (\n",
    "        (df[\"DHI\"] < 0) |\n",
    "        (df[\"DHI\"] > max_dhi) |\n",
    "        (df[\"DHI\"] > df[\"GHI\"]) |\n",
    "        (df[\"DHI\"] > dhi_est + 30) |\n",
    "        (df[\"DHI\"] > 0.95 * df[\"GHI\"]) |\n",
    "        ((solar_position[\"zenith\"] > 90) & (df[\"DHI\"] > 5))\n",
    "    )\n",
    "    df.loc[cond_invalid_dhi, \"DHI\"] = np.nan\n",
    "\n",
    "    # Interpolación robusta para DHI\n",
    "    def interpolar_robusto(serie, limit):\n",
    "        nan_groups = serie.isna().astype(int).groupby(serie.notna().astype(int).cumsum()).sum()\n",
    "        if (nan_groups > limit).any():\n",
    "            mask = serie.isna()\n",
    "            for idx, size in nan_groups[nan_groups > limit].items():\n",
    "                mask[mask.groupby(mask.cumsum()).ngroup() == idx] = False\n",
    "            serie_interp = serie.interpolate(method='linear', limit=limit, limit_direction='both')\n",
    "            serie[mask] = serie_interp[mask]\n",
    "            return serie\n",
    "        else:\n",
    "            return serie.interpolate(method='linear', limit=limit, limit_direction='both')\n",
    "\n",
    "    df[\"DHI\"] = interpolar_robusto(df[\"DHI\"], interp_limit)\n",
    "\n",
    "    # Limpiar Tdry, Tdew, RH y Pres\n",
    "    # Definir límites más realistas para cada parámetro\n",
    "    temp_limits = {\n",
    "        'Tdry': (-10, 50),  # Limitar entre -10°C y 50°C\n",
    "        'Tdew': (-10, 50),  # Limitar entre -10°C y 50°C\n",
    "        'RH': (1, 100),     # Forzar al rango físico [1%, 100%]\n",
    "        'Pres': (760, 790)  # Limitar entre 760 y 790 hPa\n",
    "    }\n",
    "\n",
    "    for param, (min_val, max_val) in temp_limits.items():\n",
    "        df[param] = df[param].mask((df[param] < min_val) | (df[param] > max_val))\n",
    "        df[param] = df[param].interpolate(method='linear', limit=interp_limit, limit_direction='both')\n",
    "\n",
    "    # Interpolación para corregir NaNs\n",
    "    for col in ['GHI', 'DNI', 'DHI', 'Tdry', 'Tdew', 'RH', 'Pres']:\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Opcional: Rellenar cualquier NaN restante con la media de la columna\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Restaurar columnas separadas\n",
    "    df = df.reset_index()\n",
    "    df[\"Year\"] = df[\"datetime\"].dt.year\n",
    "    df[\"Month\"] = df[\"datetime\"].dt.month\n",
    "    df[\"Day\"] = df[\"datetime\"].dt.day\n",
    "    df[\"Hour\"] = df[\"datetime\"].dt.hour\n",
    "    df[\"Minute\"] = df[\"datetime\"].dt.minute\n",
    "\n",
    "    # Reordenar columnas\n",
    "    columnas_fecha = [\"Year\", \"Month\", \"Day\", \"Hour\", \"Minute\"]\n",
    "    columnas_finales = columnas_fecha + [col for col in df.columns if col not in columnas_fecha + [\"datetime\"]]\n",
    "    df = df[columnas_finales]\n",
    "\n",
    "    # --- NUEVOS OUTLIERS ESPECÍFICOS POR UBICACIÓN Y MES ---\n",
    "    if location == \"Vallenar\":\n",
    "        # GHI > 1000 entre abril y agosto\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(4, 8)) & (df[\"GHI\"] > 1000))\n",
    "        # DNI > 1150 todo el año\n",
    "        df[\"DNI\"] = df[\"DNI\"].mask(df[\"DNI\"] > 1150)\n",
    "    elif location == \"Calama\":\n",
    "        # GHI > 1000 entre mayo y julio\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(5, 7)) & (df[\"GHI\"] > 1000))\n",
    "        # DNI > 1200 todo el año\n",
    "        df[\"DNI\"] = df[\"DNI\"].mask(df[\"DNI\"] > 1200)\n",
    "    elif location == \"Salvador\":\n",
    "        # GHI > 1100 entre abril y agosto\n",
    "        df[\"GHI\"] = df[\"GHI\"].mask((df[\"Month\"].between(4, 8)) & (df[\"GHI\"] > 1100))\n",
    "    # --- FIN NUEVOS OUTLIERS ---\n",
    "\n",
    "    # Después de limpiar los datos\n",
    "    df_limpio = df  # Asegúrate de que df_limpio esté definido\n",
    "    revisar_outliers_final(df_limpio, location)\n",
    "\n",
    "    # Guardar nuevo archivo con metadatos originales\n",
    "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "    print(f\"✅ Archivo limpio guardado en: {archivo_salida}\")\n",
    "\n",
    "\n",
    "def revisar_outliers_final(df, location):\n",
    "    \"\"\"\n",
    "    Revisa la existencia de outliers y NaNs en los datos limpios.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== REVISIÓN FINAL DE OUTLIERS Y NaNs - {location} ===\")\n",
    "    outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 600}  # Límites de outliers\n",
    "\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        neg_outliers = len(df[df[col] < 0])\n",
    "        high_outliers = len(df[df[col] > outlier_limits[col]])\n",
    "        total_outliers = neg_outliers + high_outliers\n",
    "        nans = df[col].isna().sum()\n",
    "        print(f\"{col}: {total_outliers} outliers, {nans} NaNs\")\n",
    "        if total_outliers > 0:\n",
    "            print(f\"  - Negativos: {neg_outliers}\")\n",
    "            print(f\"  - Altos: {high_outliers}\")\n",
    "        if nans > 0:\n",
    "            print(f\"  - NaNs: {nans}\")\n",
    "\n",
    "\n",
    "def generar_resumen_comparativo(dfs, locations):\n",
    "    \"\"\"\n",
    "    Genera un resumen comparativo del EDA para todas las ubicaciones.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RESUMEN COMPARATIVO DE DATOS SOLARES ===\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Análisis de valores faltantes\n",
    "    print(\"\\nAnálisis de Valores Faltantes:\")\n",
    "    for df, location in zip(dfs, locations):\n",
    "        nan_counts = df[['GHI', 'DNI', 'DHI']].isna().sum()\n",
    "        print(f\"\\n{location}:\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            print(f\"  {col}: {nan_counts[col]} valores faltantes ({(nan_counts[col]/len(df))*100:.2f}%)\")\n",
    "\n",
    "    # Análisis de outliers\n",
    "    print(\"\\nAnálisis de Outliers:\")\n",
    "    outlier_limits = {'GHI': 1200, 'DNI': 1300, 'DHI': 400}  # Actualizado\n",
    "    for df, location in zip(dfs, locations):\n",
    "        print(f\"\\n{location}:\")\n",
    "        for col in ['GHI', 'DNI', 'DHI']:\n",
    "            neg_outliers = len(df[df[col] < 0])\n",
    "            high_outliers = len(df[df[col] > outlier_limits[col]])\n",
    "            total_outliers = neg_outliers + high_outliers\n",
    "            print(f\"  {col}: {total_outliers} outliers ({(total_outliers/len(df))*100:.2f}%)\")\n",
    "            if total_outliers > 0:\n",
    "                print(f\"    - Negativos: {neg_outliers}\")\n",
    "                print(f\"    - Altos: {high_outliers}\")\n",
    "\n",
    "\n",
    "def reconstruir_TMY_con_fechas_originales(archivo_corrupto, archivo_limpio, archivo_salida):\n",
    "    \"\"\"\n",
    "    Genera un nuevo archivo TMY limpio con los años y meses originales del archivo fuente,\n",
    "    pero con los valores limpios. Mantiene los metadatos y el encabezado.\n",
    "    \"\"\"\n",
    "    # Leer datos limpios\n",
    "    df_limpio = pd.read_csv(archivo_limpio, skiprows=2)\n",
    "    # Leer datos originales (sin metadatos)\n",
    "    df_original = pd.read_csv(archivo_corrupto)\n",
    "    # Reemplazar columnas de fecha por las originales\n",
    "    for col in ['Year', 'Month', 'Day', 'Hour', 'Minute']:\n",
    "        if col in df_original.columns and col in df_limpio.columns:\n",
    "            df_limpio[col] = df_original[col].values\n",
    "    # Reordenar columnas para mantener el formato\n",
    "    columnas_fecha = ['Year', 'Month', 'Day', 'Hour', 'Minute']\n",
    "    columnas_finales = columnas_fecha + [col for col in df_limpio.columns if col not in columnas_fecha]\n",
    "    df_limpio = df_limpio[columnas_finales]\n",
    "    # Leer metadatos (primeras 3 líneas del archivo limpio)\n",
    "    with open(archivo_limpio, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(3)]\n",
    "    # Guardar el nuevo archivo\n",
    "    with open(archivo_salida, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        df_limpio.to_csv(f, index=False, header=False)\n",
    "    print(f\"✅ Archivo TMY limpio con fechas originales guardado en: {archivo_salida}\")\n",
    "\n",
    "\n",
    "def scan_nan_gaps(df):\n",
    "    \"\"\"\n",
    "    Muestra cuántos NaN y los huecos más largos en GHI, DNI, DHI.\n",
    "    \"\"\"\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            nan_sequences = df[col].isna().astype(int).groupby(\n",
    "                (df[col].isna().astype(int).diff() != 0).cumsum()\n",
    "            ).cumsum()\n",
    "            max_consecutive = nan_sequences.max()\n",
    "            print(f\"{col}: {nan_count} NaNs, Max gap: {max_consecutive} hours\")\n",
    "        else:\n",
    "            print(f\"{col}: No NaNs\")\n",
    "\n",
    "\n",
    "def fill_nan_hierarchical(df):\n",
    "    \"\"\"\n",
    "    1. Interpola huecos ≤ 3 h\n",
    "    2. Rellena lo restante con el promedio Mes-Hora\n",
    "    3. Si aún falta, con el promedio anual por Hora\n",
    "    \"\"\"\n",
    "    # Interpolación para huecos pequeños\n",
    "    df.interpolate(method='linear', limit=3, limit_direction='both', inplace=True)\n",
    "\n",
    "    # Rellenar con percentil 75 Mes-Hora\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df.groupby(['Month', 'Hour'])[col].transform(lambda x: x.fillna(x.quantile(0.75)))\n",
    "\n",
    "    # Rellenar con percentil 75 anual por Hora\n",
    "    for col in ['GHI', 'DNI', 'DHI']:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            df[col] = df.groupby('Hour')[col].transform(lambda x: x.fillna(x.quantile(0.75)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_and_save_final_tmy(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Lee el archivo CSV, aplica las funciones de manejo de NaN y guarda el resultado.\n",
    "    \"\"\"\n",
    "    # Leer el archivo CSV\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        metadatos = [next(f) for _ in range(2)]  # Leer las dos primeras líneas de metadatos\n",
    "    df = pd.read_csv(file_path, skiprows=2)\n",
    "\n",
    "    # Poner la columna datetime como índice\n",
    "    df['datetime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    # Escanear huecos de NaN\n",
    "    print(f\"\\nEscaneando huecos de NaN en {file_path} antes de llenar:\")\n",
    "    scan_nan_gaps(df)\n",
    "\n",
    "    # Llenar NaN jerárquicamente\n",
    "    df = fill_nan_hierarchical(df)\n",
    "\n",
    "    # Escanear nuevamente para confirmar que no quedan NaN\n",
    "    print(f\"\\nEscaneando huecos de NaN en {file_path} después de llenar:\")\n",
    "    scan_nan_gaps(df)\n",
    "\n",
    "    # Guardar el resultado con el mismo formato TMY\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(metadatos)\n",
    "        f.write(','.join(df.columns) + '\\n')  # Escribir encabezado de columnas\n",
    "        df.to_csv(f, index=False, header=False)\n",
    "    print(f\"✅ Archivo final guardado en: {output_path}\")\n",
    "\n",
    "\n",
    "def main_tmy():\n",
    "    print(\"Iniciando procesamiento...\")\n",
    "    # Definir los metadatos para cada sitio\n",
    "    metadatos_salvador = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00001\", \"City\": \"Salvador\", \"State\": \"Atacama\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -26.2533, \"Longitude\": -69.0522, \"Time Zone\": -4, \"Elevation\": 2280\n",
    "    }\n",
    "    metadatos_calama = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00002\", \"City\": \"Calama\", \"State\": \"Antofagasta\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -22.4661, \"Longitude\": -68.9244, \"Time Zone\": -4, \"Elevation\": 2260\n",
    "    }\n",
    "    metadatos_vallenar = {\n",
    "        \"Source\": \"ExpSolar\", \"Location ID\": \"00003\", \"City\": \"Vallenar\", \"State\": \"Atacama\",\n",
    "        \"Country\": \"Chile\", \"Latitude\": -28.5766, \"Longitude\": -70.7601, \"Time Zone\": -4, \"Elevation\": 441\n",
    "    }\n",
    "    # Rutas de archivos\n",
    "    path_salvador = Path(\"salvador_corrupted.csv\")\n",
    "    path_calama = Path(\"calama_corrupted.csv\")\n",
    "    path_vallenar = Path(\"Vallenar_corrupted.csv\")\n",
    "    # Salidas\n",
    "    output_salvador = Path(\"salvador_TMY_artificial.csv\")\n",
    "    output_calama = Path(\"calama_TMY_artificial.csv\")\n",
    "    output_vallenar = Path(\"vallenar_TMY_artificial.csv\")\n",
    "    output_salvador_limpio = Path(DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\")\n",
    "    output_calama_limpio = Path(DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\")\n",
    "    output_vallenar_limpio = Path(DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\")\n",
    "\n",
    "    # Procesar cada archivo (sin generar el informe EDA aquí)\n",
    "    for location, (input_path, output_path, output_limpio, metadata) in [\n",
    "        (\"Salvador\", (path_salvador, output_salvador, output_salvador_limpio, metadatos_salvador)),\n",
    "        (\"Calama\", (path_calama, output_calama, output_calama_limpio, metadatos_calama)),\n",
    "        (\"Vallenar\", (path_vallenar, output_vallenar, output_vallenar_limpio, metadatos_vallenar))\n",
    "    ]:\n",
    "        try:\n",
    "            print(f\"\\nProcesando {location}...\")\n",
    "            print(f\"Leyendo archivo: {input_path}\")\n",
    "            df = transformar_a_tmy_con_metadatos(input_path, output_path, metadata)\n",
    "            print(f\"Transformación completada. Limpiando datos...\")\n",
    "            limpiar_TMY_completo(str(output_path), str(output_limpio), location=location)\n",
    "            print(f\"Limpieza completada. Eliminando archivos temporales...\")\n",
    "            for suffix in [\"_GHI_limpio.csv\", \"_DNI_limpio.csv\", \"_DHI_limpio.csv\"]:\n",
    "                try:\n",
    "                    os.remove(str(output_path).replace(\"_TMY_artificial.csv\", suffix))\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            print(f\"Leyendo archivo limpio...\")\n",
    "            df_limpio = pd.read_csv(output_limpio, skiprows=2)\n",
    "            print(f\"Generando gráficos...\")\n",
    "            plot_tmy_data(df_limpio, location)\n",
    "            print(f\"Realizando análisis estacional...\")\n",
    "            analisis_estacional(df_limpio, location)\n",
    "            print(f\"Eliminando archivo temporal...\")\n",
    "            try:\n",
    "                os.remove(str(output_path))\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            print(f\"Procesamiento de {location} completado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {location}: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    # Generar resumen comparativo\n",
    "    dfs_limpios = [\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\", skiprows=2),\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\", skiprows=2),\n",
    "        pd.read_csv(DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\", skiprows=2)\n",
    "    ]\n",
    "    locations = [\"Salvador\", \"Calama\", \"Vallenar\"]\n",
    "    if len(dfs_limpios) == 3:\n",
    "        print(\"\\nGenerando resumen comparativo...\")\n",
    "        generar_resumen_comparativo(dfs_limpios, locations)\n",
    "        print(\"\\nReconstruyendo archivos TMY limpios con fechas originales...\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"salvador_corrupted.csv\", DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"salvador_TMY_limpio_originales.csv\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"calama_corrupted.csv\", DATOS_LIMPIOS_DIR / \"calama_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"calama_TMY_limpio_originales.csv\")\n",
    "        reconstruir_TMY_con_fechas_originales(\"Vallenar_corrupted.csv\", DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio.csv\", DATOS_LIMPIOS_DIR / \"vallenar_TMY_limpio_originales.csv\")\n",
    "    else:\n",
    "        print(f\"\\nNo se pudo generar el resumen comparativo. Se procesaron {len(dfs_limpios)} de 3 archivos.\")\n",
    "\n",
    "    # Procesar cada archivo limpio y guardar el resultado final\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'calama_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'calama_TMY_final.csv')\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'salvador_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'salvador_TMY_final.csv')\n",
    "    process_and_save_final_tmy(DATOS_LIMPIOS_DIR / 'vallenar_TMY_limpio_originales.csv', DATOS_LIMPIOS_DIR / 'vallenar_TMY_final.csv')\n",
    "\n",
    "    # Ahora sí, generar el informe EDA con ambos DataFrames\n",
    "    for location in [\"Calama\", \"Salvador\", \"Vallenar\"]:\n",
    "        df_limpio = pd.read_csv(DATOS_LIMPIOS_DIR / f\"{location.lower()}_TMY_limpio.csv\", skiprows=2)\n",
    "        df_final = pd.read_csv(DATOS_LIMPIOS_DIR / f\"{location.lower()}_TMY_final.csv\", skiprows=2)\n",
    "        generate_eda_report(df_limpio, location, df_final=df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21518998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando procesamiento...\n",
      "\n",
      "Procesando Salvador...\n",
      "Leyendo archivo: salvador_corrupted.csv\n",
      "Transformación completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/2193541072.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISIÓN FINAL DE OUTLIERS Y NaNs - Salvador ===\n",
      "GHI: 111 outliers, 1 NaNs\n",
      "  - Negativos: 0\n",
      "  - Altos: 111\n",
      "  - NaNs: 1\n",
      "DNI: 0 outliers, 0 NaNs\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "✅ Archivo limpio guardado en: datos_limpios/salvador_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gráficos...\n",
      "Realizando análisis estacional...\n",
      "\n",
      "=== ANÁLISIS ESTACIONAL - Salvador ===\n",
      "==================================================\n",
      "\n",
      "Estadísticas por Estación:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI Máximo DNI Máximo  \\\n",
      "Verano      389.684931   541.464676    15.965579     1248.3     1222.7   \n",
      "Otoño       258.563072   406.613972    14.172056     1167.7     1203.6   \n",
      "Invierno    197.343727   337.040602    11.540444      978.0     1149.7   \n",
      "Primavera   354.186401   515.705128    13.635943     1246.4     1223.6   \n",
      "\n",
      "          DHI Máximo Horas de Sol Energía Total  \n",
      "Verano         518.2         1201     841.71945  \n",
      "Otoño          535.7         1039      570.6487  \n",
      "Invierno       164.5          970     435.73495  \n",
      "Primavera      177.5         1143      773.5431  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Salvador completado.\n",
      "\n",
      "Procesando Calama...\n",
      "Leyendo archivo: calama_corrupted.csv\n",
      "Transformación completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/2193541072.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISIÓN FINAL DE OUTLIERS Y NaNs - Calama ===\n",
      "GHI: 49 outliers, 3 NaNs\n",
      "  - Negativos: 0\n",
      "  - Altos: 49\n",
      "  - NaNs: 3\n",
      "DNI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "✅ Archivo limpio guardado en: datos_limpios/calama_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gráficos...\n",
      "Realizando análisis estacional...\n",
      "\n",
      "=== ANÁLISIS ESTACIONAL - Calama ===\n",
      "==================================================\n",
      "\n",
      "Estadísticas por Estación:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI Máximo DNI Máximo  \\\n",
      "Verano       371.25412   484.361458    16.991319     1254.0     1192.9   \n",
      "Otoño        276.07951    425.86216    14.890082     1156.1     1163.1   \n",
      "Invierno    223.032465     379.3744    11.958243      985.3     1127.4   \n",
      "Primavera   351.340911   488.193816    17.601717     1233.8     1196.4   \n",
      "\n",
      "          DHI Máximo Horas de Sol Energía Total  \n",
      "Verano         432.1         1196      801.9089  \n",
      "Otoño          349.8         1060      609.0314  \n",
      "Invierno       192.0          987     492.23265  \n",
      "Primavera      527.6         1135     767.32855  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Calama completado.\n",
      "\n",
      "Procesando Vallenar...\n",
      "Leyendo archivo: Vallenar_corrupted.csv\n",
      "Transformación completada. Limpiando datos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938/2193541072.py:359: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  serie[mask] = serie_interp[mask]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REVISIÓN FINAL DE OUTLIERS Y NaNs - Vallenar ===\n",
      "GHI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DNI: 0 outliers, 2 NaNs\n",
      "  - NaNs: 2\n",
      "DHI: 0 outliers, 0 NaNs\n",
      "✅ Archivo limpio guardado en: datos_limpios/vallenar_TMY_limpio.csv\n",
      "Limpieza completada. Eliminando archivos temporales...\n",
      "Leyendo archivo limpio...\n",
      "Generando gráficos...\n",
      "Realizando análisis estacional...\n",
      "\n",
      "=== ANÁLISIS ESTACIONAL - Vallenar ===\n",
      "==================================================\n",
      "\n",
      "Estadísticas por Estación:\n",
      "          GHI Promedio DNI Promedio DHI Promedio GHI Máximo DNI Máximo  \\\n",
      "Verano      352.003056   464.676795    21.866458     1152.6     1111.0   \n",
      "Otoño        217.90401   326.889253    16.282994     1049.1     1074.7   \n",
      "Invierno    156.989964   266.652447    14.600317      864.3     1036.3   \n",
      "Primavera   302.854806   393.443402    18.393246     1164.0     1132.5   \n",
      "\n",
      "          DHI Máximo Horas de Sol Energía Total  \n",
      "Verano         268.5         1181      760.3266  \n",
      "Otoño          260.5         1029     480.91415  \n",
      "Invierno       332.6          961     346.47685  \n",
      "Primavera      235.0         1149    661.434897  \n",
      "Eliminando archivo temporal...\n",
      "Procesamiento de Vallenar completado.\n",
      "\n",
      "Generando resumen comparativo...\n",
      "\n",
      "=== RESUMEN COMPARATIVO DE DATOS SOLARES ===\n",
      "==================================================\n",
      "\n",
      "Análisis de Valores Faltantes:\n",
      "\n",
      "Salvador:\n",
      "  GHI: 1 valores faltantes (0.01%)\n",
      "  DNI: 0 valores faltantes (0.00%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Calama:\n",
      "  GHI: 3 valores faltantes (0.03%)\n",
      "  DNI: 2 valores faltantes (0.02%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Vallenar:\n",
      "  GHI: 2 valores faltantes (0.02%)\n",
      "  DNI: 2 valores faltantes (0.02%)\n",
      "  DHI: 0 valores faltantes (0.00%)\n",
      "\n",
      "Análisis de Outliers:\n",
      "\n",
      "Salvador:\n",
      "  GHI: 111 outliers (1.27%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 111\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 5 outliers (0.06%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 5\n",
      "\n",
      "Calama:\n",
      "  GHI: 49 outliers (0.56%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 49\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 4 outliers (0.05%)\n",
      "    - Negativos: 0\n",
      "    - Altos: 4\n",
      "\n",
      "Vallenar:\n",
      "  GHI: 0 outliers (0.00%)\n",
      "  DNI: 0 outliers (0.00%)\n",
      "  DHI: 0 outliers (0.00%)\n",
      "\n",
      "Reconstruyendo archivos TMY limpios con fechas originales...\n",
      "✅ Archivo TMY limpio con fechas originales guardado en: datos_limpios/salvador_TMY_limpio_originales.csv\n",
      "✅ Archivo TMY limpio con fechas originales guardado en: datos_limpios/calama_TMY_limpio_originales.csv\n",
      "✅ Archivo TMY limpio con fechas originales guardado en: datos_limpios/vallenar_TMY_limpio_originales.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/calama_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 3 NaNs, Max gap: 1 hours\n",
      "DNI: 2 NaNs, Max gap: 1 hours\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/calama_TMY_limpio_originales.csv después de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "✅ Archivo final guardado en: datos_limpios/calama_TMY_final.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/salvador_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 1 NaNs, Max gap: 1 hours\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/salvador_TMY_limpio_originales.csv después de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "✅ Archivo final guardado en: datos_limpios/salvador_TMY_final.csv\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/vallenar_TMY_limpio_originales.csv antes de llenar:\n",
      "GHI: 2 NaNs, Max gap: 1 hours\n",
      "DNI: 2 NaNs, Max gap: 1 hours\n",
      "DHI: No NaNs\n",
      "\n",
      "Escaneando huecos de NaN en datos_limpios/vallenar_TMY_limpio_originales.csv después de llenar:\n",
      "GHI: No NaNs\n",
      "DNI: No NaNs\n",
      "DHI: No NaNs\n",
      "✅ Archivo final guardado en: datos_limpios/vallenar_TMY_final.csv\n",
      "[DEBUG] Generando reporte EDA en: reports/calama_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Calama...\n",
      "[DEBUG] Generando reporte EDA en: reports/salvador_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Salvador...\n",
      "[DEBUG] Generando reporte EDA en: reports/vallenar_eda_report.txt\n",
      "[DEBUG] Escribiendo resultados finales para Vallenar...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_tmy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5888fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
